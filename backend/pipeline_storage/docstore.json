{"docstore/metadata": {"18a70fc1-8349-4494-93f4-bb06952565fa": {"doc_hash": "8ed7f0fd5bd6d741dac716d18b76d019792f2105a5de50fe94e76d3644fbf1ca"}, "2976c34a-bcf3-4856-80f9-382ee4c67a2f": {"doc_hash": "2c55b5e410802aa1e1ae30a8f7d5606f76659b22e0cbd8cd153dba80db3fb6d7"}, "9b99189f-c0d0-4505-8516-a5c931e47a2e": {"doc_hash": "298a092366c928a496a5bd495f9d4a1efd936ed3a4da38e9188dac6215bf1d46"}, "263f3dac-67e7-422a-8117-6ce96207164e": {"doc_hash": "943a2638e869efa4339583251e6eb8aa6169a31efe615f161324da525149274d"}, "d41e1291-e158-4a06-ac8c-dee04a0843fe": {"doc_hash": "bf7d4dec10010352ea018e28c00f4ff73c660231fe6294f907307cd4784a5736"}, "0134996b-0f90-400d-be23-545e5047da9b": {"doc_hash": "18c8cce1f66a6828acb28290939a453356e55cee981c979bd933263266fd41bb"}, "94bd2d09-f761-4218-8e05-c6ae44c846d3": {"doc_hash": "db3ff1d821f9f4d311bcdebbe2957974c64d08d0c0391294ea0147068b61de50"}, "dbd5d3fe-318c-4915-83b6-3b4ff1388698": {"doc_hash": "04215bc8672fad7ba4a31c046b931cd7bef66f8cd05dfd324d956b7a468c6c6c"}, "161c8439-43d5-461f-9db0-6fdff225c28d": {"doc_hash": "68aebdebef7c1d25252fba9c725987ad17fa4a2d17e297dbee17ad697e5a5c24"}, "851737d2-6767-4b95-807f-6fd67dfb351f": {"doc_hash": "cb74517b3997205d59fb6837db02213dc41b8c41a10d994b1a74f34b9b845420"}, "7762a93e-8e81-442a-bc26-e20dcb1b655f": {"doc_hash": "b7979b1e7bee72a22a92692d0893f32de2b1460f61b31ea990283987cb63f4de"}, "ef8d9f5d-9cd4-44ab-b645-0fb2fdfd288a": {"doc_hash": "a31b3d42cafcf76edddf64bbf43f4cde323673ea36a7eadcd316303b9bec807a"}, "95b68d7f-3cec-4f35-872c-b3f412f7638d": {"doc_hash": "42cf08c156e64f811ad26062eaf0375e8e87aae3490ccb340118b47cb4a991b3"}, "f585fe40-7237-46ea-90d5-de1a9b3959c6": {"doc_hash": "cd9d37f1ad48534af41b5bbb167c2b03c36088f923b335a7adcf776a19f38525"}, "4ad549a9-5c18-4c3d-a77c-69bd105b04de": {"doc_hash": "244bf9e464ef353d473155ec30935fabd6777e0db4b4fbf338b90db7b09da4ed"}, "2d11929c-a492-4b07-b7db-6ab29ce68f7f": {"doc_hash": "df4a848f75bdab82ee46836f241f268dc729b2873e99567165aebeef481d187e"}, "484846b3-49fd-46bf-88bd-6b592fe52094": {"doc_hash": "93b7ba7853387a95273e2d80ed419caa0169416f644af5bde168dbb0606f0027"}, "f38ba9b5-9961-4ec3-9039-adffbbcc15b3": {"doc_hash": "5b103cca3ea6f3029fb98ff2fa1ff85d6f57ce59685b3c5cd3c3e9e332859000"}, "af736b55-a9f0-4791-8116-f4d6eb12e165": {"doc_hash": "bc930dd644b0d4c803213708f3c5963fcf3ca5930af32097a0c36891bc3521ba"}, "c0845fb6-c8d6-40d3-a243-1f3264d09eae": {"doc_hash": "5aa047f67ad1819655ee6f4da3b3f87365c33a003c3b8922c054f64ba89ad69f"}, "3466aaa3-47d8-4e56-afdc-adcdb1b868f3": {"doc_hash": "9f2f72ec07e8243767eac5c41c49359eac7f6f59bdd60580638c2dbd21e25e5a"}, "6cf93100-b41e-450f-8fef-720f15fc35c5": {"doc_hash": "bd4efe0c5c0f1b1e4cd0eef3ef65b71303d2f7077c0f2673cf78d5bdb91fdb7b"}, "4f5d6328-d034-4ace-835f-25e43baf265d": {"doc_hash": "f3646b6474665934bf4f8b14b355d36d62943601698a414c41d4d18d11af778b"}, "9fc450ec-2b03-478f-b162-282c84941865": {"doc_hash": "eb9dbd216e287291619f66a69b457ad92e947104d87c10a079574a19fa2d1926"}, "25563a44-a4e0-4734-a15d-7f6a94ab369b": {"doc_hash": "e6b18aaac3472eb4da65ed0e051fd3315fb8d0f00416fe1706ae54e50e6783e9"}, "bcac6100-0e77-4fc3-9ed8-822567657870": {"doc_hash": "16f403926713fe9e9b8c25cb6318490c1fecffffa5cd74d6429b62ad693132e8"}, "3ad25922-dde9-4b1d-b549-8a069f71e533": {"doc_hash": "abe475eda69e541f249f8b8164da9075914f7dc5ad84a784204c1bfcb0f86ea9"}, "3c9685c2-39f8-46ad-aa17-c11605ed2ca6": {"doc_hash": "6d7cf484e18eba99af5fe9a0940bd8f2448f9dc457dedb681506285623e79de7"}, "00e889f4-6482-4e88-a8c5-dacbea93cb9e": {"doc_hash": "b9c146397fb750d56db85d5a834075b57246c6524dabff18281cb2ca50252c4c"}, "f04b9979-e6a0-4646-b1b7-68412e205589": {"doc_hash": "410fc892fde19400ce25489692db9e15da2bb3ed695b260aab2b912597b67342"}, "78e45fa1-2304-4ec1-a5d9-2524c3bc1fcb": {"doc_hash": "c86f47767357ded95e662c9c848281086e4b51652287fe32b2caa04a354eeb9b"}, "a1d0e3e1-17f5-43ee-9278-04ea6251b57c": {"doc_hash": "8dc45ab876b8b8d5c6ca433a05303862011f198a495c256423ff3ad35088fb8e"}, "48787b87-935d-4639-ac1d-e9a17ce1b778": {"doc_hash": "fb528c3937b4d0626df01c9d5314586332aab43ce9128d0a24a1b66ef4614c7a"}, "ff8eecf2-bc51-4151-88e5-7861718b6b5d": {"doc_hash": "e219ddc3dddac5f6d11d2d1225e8a19e87d846fd604acb954897a71be214cf4d"}, "2980ff31-8d0d-42a6-b022-3cda447db084": {"doc_hash": "9dbf86fad38ad520664ea2257283cd5c972796575682bf879a57593a99988973"}, "44e1fac8-661c-43be-bbc9-6505eeb04fed": {"doc_hash": "494e6f9641ef1c578cf59d6f93f1b51c2971c03dbe16ba94f2b7a2e115cb355c"}, "83ed8fd4-28da-4a3a-90e0-441ba17a4874": {"doc_hash": "3c4bc7fdf1302e28b1a85c82ff2f30d2ea3638413ad3cd357bbef16d14457006"}, "e08436f6-0cae-4ac6-ac76-08897f3cbba0": {"doc_hash": "31c4e450b3ca13740c233a6fbbcff56cf87bb13bffe30fe44f068b77c3b7f9bc"}, "ff0d576b-cb6a-4b1b-8e2f-68b58d4d3e81": {"doc_hash": "169476394bf64e6b2599d18594933677b8ce1aac2eb98ce631a079ec32664f00"}, "b7ff3d30-46c4-4a25-831a-11aef9044a70": {"doc_hash": "67086deb126811a7c3740b605907ff28d72b558c0c5407727d7504b2c4ba667a"}, "770009bf-4432-4d86-b50b-e13fb47ba24a": {"doc_hash": "f0cd5b8ca4b6177db9ceed8fb56aab087c185bbdb758e0e4c5904e10bb336812"}, "d453d912-94f1-4501-9408-ecb7ec0b6c18": {"doc_hash": "47847ea1864bcd87799a754a8465a1d92996012715dfc0844df1112980acca5a"}, "403b5c21-3869-40c2-8b2c-e9e7f3225494": {"doc_hash": "716dbe4df747b7473736bd17f16dac0ca0cfc40ad2a59b47d262a7316b132221"}, "bc48899f-3ba2-45c4-8fd4-c056409546e6": {"doc_hash": "5c3ef6f90602a52863db08128b229d647a1ea8505f2262d6e19eeeba94091267"}, "ec8b5b98-cb1b-4b74-9411-cfe6ca5eae16": {"doc_hash": "bed2cbfd4a4e2fb85dc3eba95024c7fa7278b257b40d694dbaf0c4e769bdb424"}}, "docstore/data": {"18a70fc1-8349-4494-93f4-bb06952565fa": {"__data__": {"id_": "18a70fc1-8349-4494-93f4-bb06952565fa", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/agent/index", "file_name": "index.md", "file_size": 649, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains the concept of agents, which are automated reasoning engines that handle user queries. It details agent components like breaking down questions, selecting tools, task planning, and task memory. Users can utilize built-in agents like the OpenAI Agent or create custom ones for various tasks."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Agents\n\nAn \u201cagent\u201d is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing that query in order to return the correct result. The key agent components can include, but are not limited to:\n\n- Breaking down a complex question into smaller ones\n- Choosing an external Tool to use + coming up with parameters for calling the Tool\n- Planning out a set of tasks\n- Storing previously completed tasks in a memory module\n\n## Getting Started\n\nLlamaIndex.TS comes with a few built-in agents, but you can also create your own. The built-in agents include:\n\n- OpenAI Agent\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "2976c34a-bcf3-4856-80f9-382ee4c67a2f": {"__data__": {"id_": "2976c34a-bcf3-4856-80f9-382ee4c67a2f", "embedding": null, "metadata": {"filename": "multi_document_agent.mdx", "extension": ".mdx", "file_path": "modules/agent/multi_document_agent", "file_name": "multi_document_agent.mdx", "file_size": 8190, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation outlines setting up a multi-document agent for answering various types of questions over a collection of documents. It covers installing libraries, downloading and storing data, creating document agents for each country, building a top-level agent, and utilizing the agent to answer questions. The process involves setting up tools for document agents and a top-level agent for effective information retrieval and answering queries."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Multi-Document Agent\n\nIn this guide, you learn towards setting up an agent that can effectively answer different types of questions over a larger set of documents.\n\nThese questions include the following\n\n- QA over a specific doc\n- QA comparing different docs\n- Summaries over a specific doc\n- Comparing summaries between different docs\n\nWe do this with the following architecture:\n\n- setup a \u201cdocument agent\u201d over each Document: each doc agent can do QA/summarization within its doc\n- setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question.\n\n## Setup and Download Data\n\nWe first start by installing the necessary libraries and downloading the data.\n\n```bash\npnpm i llamaindex\n```\n\n```ts\nimport {\n  Document,\n  ObjectIndex,\n  OpenAI,\n  OpenAIAgent,\n  QueryEngineTool,\n  SimpleNodeParser,\n  SimpleToolNodeMapping,\n  SummaryIndex,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n  storageContextFromDefaults,\n} from \"llamaindex\";\n```\n\nAnd then for the data we will run through a list of countries and download the wikipedia page for each country.\n\n```ts\nimport fs from \"fs\";\nimport path from \"path\";\n\nconst dataPath = path.join(__dirname, \"tmp_data\");\n\nconst extractWikipediaTitle = async (title: string) => {\n  const fileExists = fs.existsSync(path.join(dataPath, `${title}.txt`));\n\n  if (fileExists) {\n    console.log(`File already exists for the title: ${title}`);\n    return;\n  }\n\n  const queryParams = new URLSearchParams({\n    action: \"query\",\n    format: \"json\",\n    titles: title,\n    prop: \"extracts\",\n    explaintext: \"true\",\n  });\n\n  const url = `https://en.wikipedia.org/w/api.php?${queryParams}`;\n\n  const response = await fetch(url);\n  const data: any = await response.json();\n\n  const pages = data.query.pages;\n  const page = pages[Object.keys(pages)[0]];\n  const wikiText = page.extract;\n\n  await new Promise((resolve) => {\n    fs.writeFile(path.join(dataPath, `${title}.txt`), wikiText, (err: any) => {\n      if (err) {\n        console.error(err);\n        resolve(title);\n        return;\n      }\n      console.log(`${title} stored in file!`);\n\n      resolve(title);\n    });\n  });\n};\n```\n\n```ts\nexport const extractWikipedia = async (titles: string[]) => {\n  if (!fs.existsSync(dataPath)) {\n    fs.mkdirSync(dataPath);\n  }\n\n  for await (const title of titles) {\n    await extractWikipediaTitle(title);\n  }\n\n  console.log(\"Extration finished!\");\n```\n\nThese files will be saved in the `tmp_data` folder.\n\nNow we can call the function to download the data for each country.\n\n```ts\nawait extractWikipedia([\n  \"Brazil\",\n  \"United States\",\n  \"Canada\",\n  \"Mexico\",\n  \"Argentina\",\n  \"Chile\",\n  \"Colombia\",\n  \"Peru\",\n  \"Venezuela\",\n  \"Ecuador\",\n  \"Bolivia\",\n  \"Paraguay\",\n  \"Uruguay\",\n  \"Guyana\",\n  \"Suriname\",\n  \"French Guiana\",\n  \"Falkland Islands\",\n]);\n```\n\n## Load the data\n\nNow that we have the data, we can load it into the LlamaIndex and store as a document.\n\n```ts\nimport { Document } from \"llamaindex\";\n\nconst countryDocs: Record<string, Document> = {};\n\nfor (const title of wikiTitles) {\n  const path = `./agent/helpers/tmp_data/${title}.txt`;\n  const text = await fs.readFile(path, \"utf-8\");\n  const document = new Document({ text: text, id_: path });\n  countryDocs[title] = document;\n}\n```\n\n## Setup LLM and StorageContext\n\nWe will be using gpt-4 for this example and we will use the `StorageContext` to store the documents in-memory.\n\n```ts\nconst llm = new OpenAI({\n  model: \"gpt-4\",\n});\n\nconst ctx = serviceContextFromDefaults({ llm });\n\nconst storageContext = await storageContextFromDefaults({\n  persistDir: \"./storage\",\n});\n```\n\n## Building Multi-Document Agents\n\nIn this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index.\n\n```ts\nconst documentAgents: Record<string, any> = {};\nconst queryEngines: Record<string, any> = {};\n```\n\nNow we iterate over each country and create a document agent for each one.\n\n### Build Agent for each Document\n\nIn this section we define \u201cdocument agents\u201d for each document.\n\nWe define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.\n\nThis document agent can dynamically choose to perform semantic search or summarization within a given document.\n\nWe create a separate document agent for each coutnry.\n\n```ts\nfor (const title of wikiTitles) {\n  // parse the document into nodes\n  const nodes = new SimpleNodeParser({\n    chunkSize: 200,\n    chunkOverlap: 20,\n  }).getNodesFromDocuments([countryDocs[title]]);\n\n  // create the vector index for specific search\n  const vectorIndex = await VectorStoreIndex.init({\n    serviceContext: serviceContext,\n    storageContext: storageContext,\n    nodes,\n  });\n\n  // create the summary index for broader search\n  const summaryIndex = await SummaryIndex.init({\n    serviceContext: serviceContext,\n    nodes,\n  });\n\n  const vectorQueryEngine = summaryIndex.asQueryEngine();\n  const summaryQueryEngine = summaryIndex.asQueryEngine();\n\n  // create the query engines for each task\n  const queryEngineTools = [\n    new QueryEngineTool({\n      queryEngine: vectorQueryEngine,\n      metadata: {\n        name: \"vector_tool\",\n        description: `Useful for questions related to specific aspects of ${title} (e.g. the history, arts and culture, sports, demographics, or more).`,\n      },\n    }),\n    new QueryEngineTool({\n      queryEngine: summaryQueryEngine,\n      metadata: {\n        name: \"summary_tool\",\n        description: `Useful for any requests that require a holistic summary of EVERYTHING about ${title}. For questions about more specific sections, please use the vector_tool.`,\n      },\n    }),\n  ];\n\n  // create the document agent\n  const agent = new OpenAIAgent({\n    tools: queryEngineTools,\n    llm,\n    verbose: true,\n  });\n\n  documentAgents[title] = agent;\n  queryEngines[title] = vectorIndex.asQueryEngine();\n}\n```\n\n## Build Top-Level Agent\n\nNow we define the top-level agent that can answer questions over the set of document agents.\n\nThis agent takes in all document agents as tools. This specific agent RetrieverOpenAIAgent performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).\n\nHere we use a top-k retriever, but we encourage you to customize the tool retriever method!\n\nFirstly, we create a tool for each document agent\n\n```ts\nconst allTools: QueryEngineTool[] = [];\n```\n\n```ts\nfor (const title of wikiTitles) {\n  const wikiSummary = `\n    This content contains Wikipedia articles about ${title}.\n    Use this tool if you want to answer any questions about ${title}\n  `;\n\n  const docTool = new QueryEngineTool({\n    queryEngine: documentAgents[title],\n    metadata: {\n      name: `tool_${title}`,\n      description: wikiSummary,\n    },\n  });\n\n  allTools.push(docTool);\n}\n```\n\nOur top level agent will use this document agents as tools and use toolRetriever to retrieve the best tool to answer a question.\n\n```ts\n// map the tools to nodes\nconst toolMapping = SimpleToolNodeMapping.fromObjects(allTools);\n\n// create the object index\nconst objectIndex = await ObjectIndex.fromObjects(\n  allTools,\n  toolMapping,\n  VectorStoreIndex,\n  {\n    serviceContext,\n    storageContext,\n  },\n);\n\n// create the top agent\nconst topAgent = new OpenAIAgent({\n  toolRetriever: await objectIndex.asRetriever({}),\n  llm,\n  verbose: true,\n  prefixMessages: [\n    {\n      content:\n        \"You are an agent designed to answer queries about a set of given countries. Please always use the tools provided to answer a question. Do not rely on prior knowledge.\",\n      role: \"system\",\n    },\n  ],\n});\n```\n\n## Use the Agent\n\nNow we can use the agent to answer questions.\n\n```ts\nconst response = await topAgent.chat({\n  message: \"Tell me the differences between Brazil and Canada economics?\",\n});\n\n// print output\nconsole.log(response);\n```\n\nYou can find the full code for this example here\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "9b99189f-c0d0-4505-8516-a5c931e47a2e": {"__data__": {"id_": "9b99189f-c0d0-4505-8516-a5c931e47a2e", "embedding": null, "metadata": {"filename": "openai.mdx", "extension": ".mdx", "file_path": "modules/agent/openai", "file_name": "openai.mdx", "file_size": 3985, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page introduces the OpenAI Agent API for building custom agents. It demonstrates creating function tools for summing and dividing numbers, utilizing JSON schemas for parameters. The tutorial guides setting up the agent, chatting with it to perform calculations, and includes the full code for reference."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 0\n---\n\n# OpenAI Agent\n\nOpenAI API that supports function calling, it\u2019s never been easier to build your own agent!\n\nIn this notebook tutorial, we showcase how to write your own OpenAI agent\n\n## Setup\n\nFirst, you need to install the `llamaindex` package. You can do this by running the following command in your terminal:\n\n```bash\npnpm i llamaindex\n```\n\nThen we can define a function to sum two numbers and another function to divide two numbers.\n\n```ts\nfunction sumNumbers({ a, b }: { a: number; b: number }): number {\n  return a + b;\n}\n\n// Define a function to divide two numbers\nfunction divideNumbers({ a, b }: { a: number; b: number }): number {\n  return a / b;\n}\n```\n\n## Create a function tool\n\nNow we can create a function tool from the sum function and another function tool from the divide function.\n\nFor the parameters of the sum function, we can define a JSON schema.\n\n### JSON Schema\n\n```ts\nconst sumJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The first number\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The second number\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\nconst divideJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The dividend a to divide\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The divisor b to divide by\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\nconst sumFunctionTool = new FunctionTool(sumNumbers, {\n  name: \"sumNumbers\",\n  description: \"Use this function to sum two numbers\",\n  parameters: sumJSON,\n});\n\nconst divideFunctionTool = new FunctionTool(divideNumbers, {\n  name: \"divideNumbers\",\n  description: \"Use this function to divide two numbers\",\n  parameters: divideJSON,\n});\n```\n\n## Create an OpenAIAgent\n\nNow we can create an OpenAIAgent with the function tools.\n\n```ts\nconst agent = new OpenAIAgent({\n  tools: [sumFunctionTool, divideFunctionTool],\n  verbose: true,\n});\n```\n\n## Chat with the agent\n\nNow we can chat with the agent.\n\n```ts\nconst response = await agent.chat({\n  message: \"How much is 5 + 5? then divide by 2\",\n});\n\nconsole.log(String(response));\n```\n\n## Full code\n\n```ts\nimport { FunctionTool, OpenAIAgent } from \"llamaindex\";\n\n// Define a function to sum two numbers\nfunction sumNumbers({ a, b }: { a: number; b: number }): number {\n  return a + b;\n}\n\n// Define a function to divide two numbers\nfunction divideNumbers({ a, b }: { a: number; b: number }): number {\n  return a / b;\n}\n\n// Define the parameters of the sum function as a JSON schema\nconst sumJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The first number\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The second number\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\n// Define the parameters of the divide function as a JSON schema\nconst divideJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The argument a to divide\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The argument b to divide\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\nasync function main() {\n  // Create a function tool from the sum function\n  const sumFunctionTool = new FunctionTool(sumNumbers, {\n    name: \"sumNumbers\",\n    description: \"Use this function to sum two numbers\",\n    parameters: sumJSON,\n  });\n\n  // Create a function tool from the divide function\n  const divideFunctionTool = new FunctionTool(divideNumbers, {\n    name: \"divideNumbers\",\n    description: \"Use this function to divide two numbers\",\n    parameters: divideJSON,\n  });\n\n  // Create an OpenAIAgent with the function tools\n  const agent = new OpenAIAgent({\n    tools: [sumFunctionTool, divideFunctionTool],\n    verbose: true,\n  });\n\n  // Chat with the agent\n  const response = await agent.chat({\n    message: \"How much is 5 + 5? then divide by 2\",\n  });\n\n  // Print the response\n  console.log(String(response));\n}\n\nmain().then(() => {\n  console.log(\"Done\");\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "263f3dac-67e7-422a-8117-6ce96207164e": {"__data__": {"id_": "263f3dac-67e7-422a-8117-6ce96207164e", "embedding": null, "metadata": {"filename": "query_engine_tool.mdx", "extension": ".mdx", "file_path": "modules/agent/query_engine_tool", "file_name": "query_engine_tool.mdx", "file_size": 2999, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation page guides users on setting up a QueryEngineTool to query a vector index created from documents using the LlamaIndex package. It demonstrates creating a vector index, a query engine, a QueryEngineTool, and an OpenAIAgent to interact with the agent through chat, showcasing practical implementation steps."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 1\n---\n\n# OpenAI Agent + QueryEngineTool\n\nQueryEngineTool is a tool that allows you to query a vector index. In this example, we will create a vector index from a set of documents and then create a QueryEngineTool from the vector index. We will then create an OpenAIAgent with the QueryEngineTool and chat with the agent.\n\n## Setup\n\nFirst, you need to install the `llamaindex` package. You can do this by running the following command in your terminal:\n\n```bash\npnpm i llamaindex\n```\n\nThen you can import the necessary classes and functions.\n\n```ts\nimport {\n  OpenAIAgent,\n  SimpleDirectoryReader,\n  VectorStoreIndex,\n  QueryEngineTool,\n} from \"llamaindex\";\n```\n\n## Create a vector index\n\nNow we can create a vector index from a set of documents.\n\n```ts\n// Load the documents\nconst documents = await new SimpleDirectoryReader().loadData({\n  directoryPath: \"node_modules/llamaindex/examples/\",\n});\n\n// Create a vector index from the documents\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n```\n\n## Create a QueryEngineTool\n\nNow we can create a QueryEngineTool from the vector index.\n\n```ts\n// Create a query engine from the vector index\nconst abramovQueryEngine = vectorIndex.asQueryEngine();\n\n// Create a QueryEngineTool with the query engine\nconst queryEngineTool = new QueryEngineTool({\n  queryEngine: abramovQueryEngine,\n  metadata: {\n    name: \"abramov_query_engine\",\n    description: \"A query engine for the Abramov documents\",\n  },\n});\n```\n\n## Create an OpenAIAgent\n\n```ts\n// Create an OpenAIAgent with the query engine tool tools\n\nconst agent = new OpenAIAgent({\n  tools: [queryEngineTool],\n  verbose: true,\n});\n```\n\n## Chat with the agent\n\nNow we can chat with the agent.\n\n```ts\nconst response = await agent.chat({\n  message: \"What was his salary?\",\n});\n\nconsole.log(String(response));\n```\n\n## Full code\n\n```ts\nimport {\n  OpenAIAgent,\n  SimpleDirectoryReader,\n  VectorStoreIndex,\n  QueryEngineTool,\n} from \"llamaindex\";\n\nasync function main() {\n  // Load the documents\n  const documents = await new SimpleDirectoryReader().loadData({\n    directoryPath: \"node_modules/llamaindex/examples/\",\n  });\n\n  // Create a vector index from the documents\n  const vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n\n  // Create a query engine from the vector index\n  const abramovQueryEngine = vectorIndex.asQueryEngine();\n\n  // Create a QueryEngineTool with the query engine\n  const queryEngineTool = new QueryEngineTool({\n    queryEngine: abramovQueryEngine,\n    metadata: {\n      name: \"abramov_query_engine\",\n      description: \"A query engine for the Abramov documents\",\n    },\n  });\n\n  // Create an OpenAIAgent with the function tools\n  const agent = new OpenAIAgent({\n    tools: [queryEngineTool],\n    verbose: true,\n  });\n\n  // Chat with the agent\n  const response = await agent.chat({\n    message: \"What was his salary?\",\n  });\n\n  // Print the response\n  console.log(String(response));\n}\n\nmain().then(() => {\n  console.log(\"Done\");\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "d41e1291-e158-4a06-ac8c-dee04a0843fe": {"__data__": {"id_": "d41e1291-e158-4a06-ac8c-dee04a0843fe", "embedding": null, "metadata": {"filename": "react_agent.mdx", "extension": ".mdx", "file_path": "modules/agent/react_agent", "file_name": "react_agent.mdx", "file_size": 4591, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "Learn to create a ReAct agent using the `llamaindex` package to automate tasks. Define functions like sum and divide, create function tools with JSON schemas, and interact with the agent to perform calculations. The agent reasons, executes actions, and provides answers based on defined tools, showcasing AI automation capabilities."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# ReAct Agent\n\nThe ReAct agent is an AI agent that can reason over the next action, construct an action command, execute the action, and repeat these steps in an iterative loop until the task is complete.\n\nIn this notebook tutorial, we showcase how to write your ReAct agent using the `llamaindex` package.\n\n## Setup\n\nFirst, you need to install the `llamaindex` package. You can do this by running the following command in your terminal:\n\n```bash\npnpm i llamaindex\n```\n\nAnd then you can import the `OpenAIAgent` and `FunctionTool` from the `llamaindex` package.\n\n```ts\nimport { FunctionTool, OpenAIAgent } from \"llamaindex\";\n```\n\nThen we can define a function to sum two numbers and another function to divide two numbers.\n\n```ts\nfunction sumNumbers({ a, b }: { a: number; b: number }): number {\n  return a + b;\n}\n\n// Define a function to divide two numbers\nfunction divideNumbers({ a, b }: { a: number; b: number }): number {\n  return a / b;\n}\n```\n\n## Create a function tool\n\nNow we can create a function tool from the sum function and another function tool from the divide function.\n\nFor the parameters of the sum function, we can define a JSON schema.\n\n### JSON Schema\n\n```ts\nconst sumJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The first number\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The second number\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\nconst divideJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The dividend a to divide\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The divisor b to divide by\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\nconst sumFunctionTool = new FunctionTool(sumNumbers, {\n  name: \"sumNumbers\",\n  description: \"Use this function to sum two numbers\",\n  parameters: sumJSON,\n});\n\nconst divideFunctionTool = new FunctionTool(divideNumbers, {\n  name: \"divideNumbers\",\n  description: \"Use this function to divide two numbers\",\n  parameters: divideJSON,\n});\n```\n\n## Create an ReAct\n\nNow we can create an OpenAIAgent with the function tools.\n\n```ts\nconst agent = new ReActAgent({\n  tools: [sumFunctionTool, divideFunctionTool],\n  verbose: true,\n});\n```\n\n## Chat with the agent\n\nNow we can chat with the agent.\n\n```ts\nconst response = await agent.chat({\n  message: \"How much is 5 + 5? then divide by 2\",\n});\n\nconsole.log(String(response));\n```\n\nThe output will be:\n\n```bash\nThought: I need to use a tool to help me answer the question.\nAction: sumNumbers\nAction Input: {\"a\":5,\"b\":5}\n\nObservation: 10\nThought: I can answer without using any more tools.\nAnswer: The sum of 5 and 5 is 10, and when divided by 2, the result is 5.\n\nThe sum of 5 and 5 is 10, and when divided by 2, the result is 5.\n```\n\n## Full code\n\n```ts\nimport { FunctionTool, ReActAgent } from \"llamaindex\";\n\n// Define a function to sum two numbers\nfunction sumNumbers({ a, b }: { a: number; b: number }): number {\n  return a + b;\n}\n\n// Define a function to divide two numbers\nfunction divideNumbers({ a, b }: { a: number; b: number }): number {\n  return a / b;\n}\n\n// Define the parameters of the sum function as a JSON schema\nconst sumJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The first number\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The second number\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\n// Define the parameters of the divide function as a JSON schema\nconst divideJSON = {\n  type: \"object\",\n  properties: {\n    a: {\n      type: \"number\",\n      description: \"The argument a to divide\",\n    },\n    b: {\n      type: \"number\",\n      description: \"The argument b to divide\",\n    },\n  },\n  required: [\"a\", \"b\"],\n};\n\nasync function main() {\n  // Create a function tool from the sum function\n  const sumFunctionTool = new FunctionTool(sumNumbers, {\n    name: \"sumNumbers\",\n    description: \"Use this function to sum two numbers\",\n    parameters: sumJSON,\n  });\n\n  // Create a function tool from the divide function\n  const divideFunctionTool = new FunctionTool(divideNumbers, {\n    name: \"divideNumbers\",\n    description: \"Use this function to divide two numbers\",\n    parameters: divideJSON,\n  });\n\n  // Create an OpenAIAgent with the function tools\n  const agent = new OpenAIAgent({\n    tools: [sumFunctionTool, divideFunctionTool],\n    verbose: true,\n  });\n\n  // Chat with the agent\n  const response = await agent.chat({\n    message: \"I want to sum 5 and 5 and then divide by 2\",\n  });\n\n  // Print the response\n  console.log(String(response));\n}\n\nmain().then(() => {\n  console.log(\"Done\");\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "0134996b-0f90-400d-be23-545e5047da9b": {"__data__": {"id_": "0134996b-0f90-400d-be23-545e5047da9b", "embedding": null, "metadata": {"filename": "chat_engine.md", "extension": ".md", "file_path": "modules/chat_engine", "file_name": "chat_engine.md", "file_size": 716, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation page provides guidance on using the ChatEngine to interact with index data. It demonstrates initializing the engine, starting a chat session with a query, and enabling streaming for continuous responses. The page references ContextChatEngine and CondenseQuestionChatEngine for further exploration."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 4\n---\n\n# ChatEngine\n\nThe chat engine is a quick and simple way to chat with the data in your index.\n\n```typescript\nconst retriever = index.asRetriever();\nconst chatEngine = new ContextChatEngine({ retriever });\n\n// start chatting\nconst response = await chatEngine.chat({ message: query });\n```\n\nThe `chat` function also supports streaming, just add `stream: true` as an option:\n\n```typescript\nconst stream = await chatEngine.chat({ message: query, stream: true });\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.response);\n}\n```\n\n## Api References\n\n- ContextChatEngine\n- CondenseQuestionChatEngine\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "94bd2d09-f761-4218-8e05-c6ae44c846d3": {"__data__": {"id_": "94bd2d09-f761-4218-8e05-c6ae44c846d3", "embedding": null, "metadata": {"filename": "data_index.md", "extension": ".md", "file_path": "modules/data_index", "file_name": "data_index.md", "file_size": 667, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains two types of indexes: VectorStoreIndex sends top-k Nodes to the LLM for responses, while SummaryIndex sends all Nodes. The code snippet demonstrates creating a Document and initializing a VectorStoreIndex. The API Reference lists SummaryIndex and VectorStoreIndex for further details."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 4\n---\n\n# Index\n\nAn index is the basic container and organization for your data. LlamaIndex.TS supports two indexes:\n\n- `VectorStoreIndex` - will send the top-k `Node`s to the LLM when generating a response. The default top-k is 2.\n- `SummaryIndex` - will send every `Node` in the index to the LLM in order to generate a response\n\n```typescript\nimport { Document, VectorStoreIndex } from \"llamaindex\";\n\nconst document = new Document({ text: \"test\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document]);\n```\n\n## API Reference\n\n- SummaryIndex\n- VectorStoreIndex\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "dbd5d3fe-318c-4915-83b6-3b4ff1388698": {"__data__": {"id_": "dbd5d3fe-318c-4915-83b6-3b4ff1388698", "embedding": null, "metadata": {"filename": "data_loader.mdx", "extension": ".mdx", "file_path": "modules/data_loader", "file_name": "data_loader.mdx", "file_size": 2071, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page covers loading files into memory using the SimpleDirectoryReader for various file types like .csv, .docx, .html, .md, and .pdf. It also introduces LlamaParse for efficient file parsing, especially converting PDF tables into markdown. Users can customize file readers and utilize the API for document conversion."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 4\n---\n\nimport CodeBlock from \"@theme/CodeBlock\";\nimport CodeSource from \"!raw-loader!../../../../examples/readers/src/simple-directory-reader\";\nimport CodeSource2 from \"!raw-loader!../../../../examples/readers/src/custom-simple-directory-reader\";\nimport CodeSource3 from \"!raw-loader!../../../../examples/readers/src/llamaparse\";\n\n# Loader\n\nBefore you can start indexing your documents, you need to load them into memory.\n\n### SimpleDirectoryReader\n\n![Open in StackBlitz](https://stackblitz.com/github/run-llama/LlamaIndexTS/tree/main/examples/readers?file=src/simple-directory-reader.ts&title=Simple%20Directory%20Reader)\n\nLlamaIndex.TS supports easy loading of files from folders using the `SimpleDirectoryReader` class.\n\nIt is a simple reader that reads all files from a directory and its subdirectories.\n\n<CodeBlock language=\"ts\">{CodeSource}</CodeBlock>\n\nCurrently, it supports reading `.csv`, `.docx`, `.html`, `.md` and `.pdf` files,\nbut support for other file types is planned.\n\nAlso, you can provide a `defaultReader` as a fallback for files with unsupported extensions.\nOr pass new readers for `fileExtToReader` to support more file types.\n\n<CodeBlock language=\"ts\" showLineNumbers metastring=\"{8-12,17-21}\">\n  {CodeSource2}\n</CodeBlock>\n\n### LlamaParse\n\nLlamaParse is an API created by LlamaIndex to efficiently parse files, e.g. it's great at converting PDF tables into markdown.\n\nTo use it, first login and get an API key from https://cloud.llamaindex.ai. Make sure to store the key in the environment variable `LLAMA_CLOUD_API_KEY`.\n\nThen, you can use the `LlamaParseReader` class to read a local PDF file and convert it into a markdown document that can be used by LlamaIndex:\n\n<CodeBlock language=\"ts\">{CodeSource3}</CodeBlock>\n\nAlternatively, you can set the `resultType` option to `text` to get the parsed document as a text string.\n\n## API Reference\n\n- SimpleDirectoryReader\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "161c8439-43d5-461f-9db0-6fdff225c28d": {"__data__": {"id_": "161c8439-43d5-461f-9db0-6fdff225c28d", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/documents_and_nodes/index", "file_name": "index.md", "file_size": 534, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page introduces `Document`s and `Node`s as fundamental components of an index. `Document`s represent entire files, while `Node`s are smaller segments suitable for Q&A. The API includes classes like `Document` and `TextNode`. An example demonstrates creating a `Document` with text and metadata."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 1\n---\n\n# Documents and Nodes\n\n`Document`s and `Node`s are the basic building blocks of any index. While the API for these objects is similar, `Document` objects represent entire files, while `Node`s are smaller pieces of that original document, that are suitable for an LLM and Q&A.\n\n```typescript\nimport { Document } from \"llamaindex\";\n\ndocument = new Document({ text: \"text\", metadata: { key: \"val\" } });\n```\n\n## API Reference\n\n- Document\n- TextNode\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "851737d2-6767-4b95-807f-6fd67dfb351f": {"__data__": {"id_": "851737d2-6767-4b95-807f-6fd67dfb351f", "embedding": null, "metadata": {"filename": "metadata_extraction.md", "extension": ".md", "file_path": "modules/documents_and_nodes/metadata_extraction", "file_name": "metadata_extraction.md", "file_size": 1240, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains how to automate metadata extraction using LLMs with modules like SummaryExtractor, QuestionsAnsweredExtractor, TitleExtractor, and KeywordExtractor. These modules can be chained with IngestionPipeline to extract metadata from documents efficiently, as demonstrated in the provided code snippet."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Metadata Extraction Usage Pattern\n\nYou can use LLMs to automate metadata extraction with our `Metadata Extractor` modules.\n\nOur metadata extractor modules include the following \"feature extractors\":\n\n- `SummaryExtractor` - automatically extracts a summary over a set of Nodes\n- `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer\n- `TitleExtractor` - extracts a title over the context of each Node by document and combine them\n- `KeywordExtractor` - extracts keywords over the context of each Node\n\nThen you can chain the `Metadata Extractors` with the `IngestionPipeline` to extract metadata from a set of documents.\n\n```ts\nimport {\n  IngestionPipeline,\n  TitleExtractor,\n  QuestionsAnsweredExtractor,\n  Document,\n  OpenAI,\n} from \"llamaindex\";\n\nasync function main() {\n  const pipeline = new IngestionPipeline({\n    transformations: [\n      new TitleExtractor(),\n      new QuestionsAnsweredExtractor({\n        questions: 5,\n      }),\n    ],\n  });\n\n  const nodes = await pipeline.run({\n    documents: [\n      new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n    ],\n  });\n\n  for (const node of nodes) {\n    console.log(node.metadata);\n  }\n}\n\nmain().then(() => console.log(\"done\"));\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "7762a93e-8e81-442a-bc26-e20dcb1b655f": {"__data__": {"id_": "7762a93e-8e81-442a-bc26-e20dcb1b655f", "embedding": null, "metadata": {"filename": "huggingface.md", "extension": ".md", "file_path": "modules/embeddings/available_embeddings/huggingface", "file_name": "huggingface.md", "file_size": 632, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The provided code snippet showcases the usage of HuggingFace embeddings in LlamaIndex. It demonstrates importing, creating embeddings, setting up service context, indexing documents, creating a query engine, and executing a query to retrieve results based on a query string."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# HuggingFace\n\nTo use HuggingFace embeddings, you need to import `HuggingFaceEmbedding` from `llamaindex`.\n\n```ts\nimport { HuggingFaceEmbedding, serviceContextFromDefaults } from \"llamaindex\";\n\nconst huggingFaceEmbeds = new HuggingFaceEmbedding();\n\nconst serviceContext = serviceContextFromDefaults({ embedModel: openaiEmbeds });\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "ef8d9f5d-9cd4-44ab-b645-0fb2fdfd288a": {"__data__": {"id_": "ef8d9f5d-9cd4-44ab-b645-0fb2fdfd288a", "embedding": null, "metadata": {"filename": "mistral.md", "extension": ".md", "file_path": "modules/embeddings/available_embeddings/mistral", "file_name": "mistral.md", "file_size": 661, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The provided code snippet demonstrates how to use MistralAI embeddings with LlamaIndex. It involves importing `MistralAIEmbedding` from `llamaindex`, creating an embedding model, setting up a service context, creating a document, building an index, setting up a query engine, and executing a query to find relevant results based on a given query string."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# MistralAI\n\nTo use MistralAI embeddings, you need to import `MistralAIEmbedding` from `llamaindex`.\n\n```ts\nimport { MistralAIEmbedding, serviceContextFromDefaults } from \"llamaindex\";\n\nconst mistralEmbedModel = new MistralAIEmbedding({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({\n  embedModel: mistralEmbedModel,\n});\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "95b68d7f-3cec-4f35-872c-b3f412f7638d": {"__data__": {"id_": "95b68d7f-3cec-4f35-872c-b3f412f7638d", "embedding": null, "metadata": {"filename": "ollama.md", "extension": ".md", "file_path": "modules/embeddings/available_embeddings/ollama", "file_name": "ollama.md", "file_size": 586, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The provided documentation introduces using Ollama embeddings through LlamaIndex. It demonstrates importing Ollama, creating a service context, setting up a document, building an index, creating a query engine, and executing a query to find the meaning of life."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Ollama\n\nTo use Ollama embeddings, you need to import `Ollama` from `llamaindex`.\n\n```ts\nimport { Ollama, serviceContextFromDefaults } from \"llamaindex\";\n\nconst ollamaEmbedModel = new Ollama();\n\nconst serviceContext = serviceContextFromDefaults({\n  embedModel: ollamaEmbedModel,\n});\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "f585fe40-7237-46ea-90d5-de1a9b3959c6": {"__data__": {"id_": "f585fe40-7237-46ea-90d5-de1a9b3959c6", "embedding": null, "metadata": {"filename": "openai.md", "extension": ".md", "file_path": "modules/embeddings/available_embeddings/openai", "file_name": "openai.md", "file_size": 613, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The provided code snippet demonstrates how to utilize OpenAI embeddings with LlamaIndex. It shows importing the necessary modules, creating an OpenAI embedding model, setting up a service context, creating a document, building an index, and querying for information. The process involves text analysis and retrieval using LlamaIndex functionalities."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# OpenAI\n\nTo use OpenAI embeddings, you need to import `OpenAIEmbedding` from `llamaindex`.\n\n```ts\nimport { OpenAIEmbedding, serviceContextFromDefaults } from \"llamaindex\";\n\nconst openaiEmbedModel = new OpenAIEmbedding();\n\nconst serviceContext = serviceContextFromDefaults({\n  embedModel: openaiEmbedModel,\n});\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "4ad549a9-5c18-4c3d-a77c-69bd105b04de": {"__data__": {"id_": "4ad549a9-5c18-4c3d-a77c-69bd105b04de", "embedding": null, "metadata": {"filename": "together.md", "extension": ".md", "file_path": "modules/embeddings/available_embeddings/together", "file_name": "together.md", "file_size": 658, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation page provides guidance on using Together embeddings with LlamaIndex. It explains importing `TogetherEmbedding` from `llamaindex`, setting up the embedding model with an API key, creating a service context, indexing a document, setting up a query engine, and executing a query to find the meaning of life."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Together\n\nTo use together embeddings, you need to import `TogetherEmbedding` from `llamaindex`.\n\n```ts\nimport { TogetherEmbedding, serviceContextFromDefaults } from \"llamaindex\";\n\nconst togetherEmbedModel = new TogetherEmbedding({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({\n  embedModel: togetherEmbedModel,\n});\n\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "2d11929c-a492-4b07-b7db-6ab29ce68f7f": {"__data__": {"id_": "2d11929c-a492-4b07-b7db-6ab29ce68f7f", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/embeddings/index", "file_name": "index.md", "file_size": 736, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation explains how to create numerical text representations using the default `text-embedding-ada-002` model from OpenAI or the HuggingFace model for local embeddings. It introduces the `OpenAIEmbedding` and `ServiceContext` classes, showing how to set the embedding model in the `ServiceContext` object for text representation tasks."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Embedding\n\nThe embedding model in LlamaIndex is responsible for creating numerical representations of text. By default, LlamaIndex will use the `text-embedding-ada-002` model from OpenAI.\n\nThis can be explicitly set in the `ServiceContext` object.\n\n```typescript\nimport { OpenAIEmbedding, serviceContextFromDefaults } from \"llamaindex\";\n\nconst openaiEmbeds = new OpenAIEmbedding();\n\nconst serviceContext = serviceContextFromDefaults({ embedModel: openaiEmbeds });\n```\n\n## Local Embedding\n\nFor local embeddings, you can use the HuggingFace embedding model.\n\n## API Reference\n\n- OpenAIEmbedding\n- ServiceContext\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "484846b3-49fd-46bf-88bd-6b592fe52094": {"__data__": {"id_": "484846b3-49fd-46bf-88bd-6b592fe52094", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/evaluation/index", "file_name": "index.md", "file_size": 1740, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page covers the importance of evaluation and benchmarking in LLM development. It discusses Response Evaluation and Retrieval Evaluation, highlighting modules like Correctness, Faithfulness, and Relevancy Evaluators to measure the quality of generated results without requiring ground-truth labels."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Evaluating\n\n## Concept\n\nEvaluation and benchmarking are crucial concepts in LLM development. To improve the perfomance of an LLM app (RAG, agents) you must have a way to measure it.\n\nLlamaIndex offers key modules to measure the quality of generated results. We also offer key modules to measure retrieval quality.\n\n- **Response Evaluation**: Does the response match the retrieved context? Does it also match the query? Does it match the reference answer or guidelines?\n- **Retrieval Evaluation**: Are the retrieved sources relevant to the query?\n\n## Response Evaluation\n\nEvaluation of generated results can be difficult, since unlike traditional machine learning the predicted result is not a single number, and it can be hard to define quantitative metrics for this problem.\n\nLlamaIndex offers LLM-based evaluation modules to measure the quality of results. This uses a \u201cgold\u201d LLM (e.g. GPT-4) to decide whether the predicted answer is correct in a variety of ways.\n\nNote that many of these current evaluation modules do not require ground-truth labels. Evaluation can be done with some combination of the query, context, response, and combine these with LLM calls.\n\nThese evaluation modules are in the following forms:\n\n- **Correctness**: Whether the generated answer matches that of the reference answer given the query (requires labels).\n\n- **Faithfulness**: Evaluates if the answer is faithful to the retrieved contexts (in other words, whether if there\u2019s hallucination).\n\n- **Relevancy**: Evaluates if the response from a query engine matches any source nodes.\n\n## Usage\n\n- Correctness Evaluator\n- Faithfulness Evaluator\n- Relevancy Evaluator\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "f38ba9b5-9961-4ec3-9039-adffbbcc15b3": {"__data__": {"id_": "f38ba9b5-9961-4ec3-9039-adffbbcc15b3", "embedding": null, "metadata": {"filename": "correctness.md", "extension": ".md", "file_path": "modules/evaluation/modules/correctness", "file_name": "correctness.md", "file_size": 2347, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The Correctness Evaluator in LlamaIndex assesses answer accuracy on a scale of 0 to 5. It requires package installation, API key setup, and module import. The example evaluates a response on Einstein's theory of relativity, explaining special and general relativity. The response provided is rated as not entirely correct with a score of 2.5."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Correctness Evaluator\n\nCorrectness evaluates the relevance and correctness of a generated answer against a reference answer.\n\nThis is useful for measuring if the response was correct. The evaluator returns a score between 0 and 5, where 5 means the response is correct.\n\n## Usage\n\nFirstly, you need to install the package:\n\n```bash\npnpm i llamaindex\n```\n\nSet the OpenAI API key:\n\n```bash\nexport OPENAI_API_KEY=your-api-key\n```\n\nImport the required modules:\n\n```ts\nimport {\n  CorrectnessEvaluator,\n  OpenAI,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n```\n\nLet's setup gpt-4 for better results:\n\n```ts\nconst llm = new OpenAI({\n  model: \"gpt-4\",\n});\n\nconst ctx = serviceContextFromDefaults({\n  llm,\n});\n```\n\n```ts\nconst query =\n  \"Can you explain the theory of relativity proposed by Albert Einstein in detail?\";\n\nconst response = ` Certainly! Albert Einstein's theory of relativity consists of two main components: special relativity and general relativity. Special relativity, published in 1905, introduced the concept that the laws of physics are the same for all non-accelerating observers and that the speed of light in a vacuum is a constant, regardless of the motion of the source or observer. It also gave rise to the famous equation E=mc\u00b2, which relates energy (E) and mass (m).\n\nHowever, general relativity, published in 1915, extended these ideas to include the effects of magnetism. According to general relativity, gravity is not a force between masses but rather the result of the warping of space and time by magnetic fields generated by massive objects. Massive objects, such as planets and stars, create magnetic fields that cause a curvature in spacetime, and smaller objects follow curved paths in response to this magnetic curvature. This concept is often illustrated using the analogy of a heavy ball placed on a rubber sheet with magnets underneath, causing it to create a depression that other objects (representing smaller masses) naturally move towards due to magnetic attraction.\n`;\n\nconst evaluator = new CorrectnessEvaluator({\n  serviceContext: ctx,\n});\n\nconst result = await evaluator.evaluateResponse({\n  query,\n  response,\n});\n\nconsole.log(\n  `the response is ${result.passing ? \"correct\" : \"not correct\"} with a score of ${result.score}`,\n);\n```\n\n```bash\nthe response is not correct with a score of 2.5\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "af736b55-a9f0-4791-8116-f4d6eb12e165": {"__data__": {"id_": "af736b55-a9f0-4791-8116-f4d6eb12e165", "embedding": null, "metadata": {"filename": "faithfulness.md", "extension": ".md", "file_path": "modules/evaluation/modules/faithfulness", "file_name": "faithfulness.md", "file_size": 2788, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The Faithfulness Evaluator in LlamaIndex measures if a response matches retrieved contexts, detecting hallucinations. It uses OpenAI for evaluation, with steps to set up GPT-4 for better results. By creating a vector index and query engine, responses can be evaluated for faithfulness, providing a score indicating fidelity to the contexts."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Faithfulness Evaluator\n\nFaithfulness is a measure of whether the generated answer is faithful to the retrieved contexts. In other words, it measures whether there is any hallucination in the generated answer.\n\nThis uses the FaithfulnessEvaluator module to measure if the response from a query engine matches any source nodes.\n\nThis is useful for measuring if the response was hallucinated. The evaluator returns a score between 0 and 1, where 1 means the response is faithful to the retrieved contexts.\n\n## Usage\n\nFirstly, you need to install the package:\n\n```bash\npnpm i llamaindex\n```\n\nSet the OpenAI API key:\n\n```bash\nexport OPENAI_API_KEY=your-api-key\n```\n\nImport the required modules:\n\n```ts\nimport {\n  Document,\n  FaithfulnessEvaluator,\n  OpenAI,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n```\n\nLet's setup gpt-4 for better results:\n\n```ts\nconst llm = new OpenAI({\n  model: \"gpt-4\",\n});\n\nconst ctx = serviceContextFromDefaults({\n  llm,\n});\n```\n\nNow, let's create a vector index and query engine with documents and query engine respectively. Then, we can evaluate the response with the query and response from the query engine.:\n\n```ts\nconst documents = [\n  new Document({\n    text: `The city came under British control in 1664 and was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city was regained by the Dutch in July 1673 and was renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States from 1785 until 1790, and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace. In the 21st century, New York City has emerged as a global node of creativity, entrepreneurship, and as a symbol of freedom and cultural diversity. The New York Times has won the most Pulitzer Prizes for journalism and remains the U.S. media's \"newspaper of record\". In 2019, New York City was voted the greatest city in the world in a survey of over 30,000 p...\tPass`,\n  }),\n];\n\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n\nconst queryEngine = vectorIndex.asQueryEngine();\n```\n\nNow, let's evaluate the response:\n\n```ts\nconst query = \"How did New York City get its name?\";\n\nconst evaluator = new FaithfulnessEvaluator({\n  serviceContext: ctx,\n});\n\nconst response = await queryEngine.query({\n  query,\n});\n\nconst result = await evaluator.evaluateResponse({\n  query,\n  response,\n});\n\nconsole.log(`the response is ${result.passing ? \"faithful\" : \"not faithful\"}`);\n```\n\n```bash\nthe response is faithful\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "c0845fb6-c8d6-40d3-a243-1f3264d09eae": {"__data__": {"id_": "c0845fb6-c8d6-40d3-a243-1f3264d09eae", "embedding": null, "metadata": {"filename": "relevancy.md", "extension": ".md", "file_path": "modules/evaluation/modules/relevancy", "file_name": "relevancy.md", "file_size": 2413, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation introduces the Relevancy Evaluator in LlamaIndex, measuring response relevance to queries. It details installation steps, setting up OpenAI, creating a vector index and query engine, and evaluating responses. An example query about New York City's name showcases the process, emphasizing relevance assessment for query responses."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Relevancy Evaluator\n\nRelevancy measure if the response from a query engine matches any source nodes.\n\nIt is useful for measuring if the response was relevant to the query. The evaluator returns a score between 0 and 1, where 1 means the response is relevant to the query.\n\n## Usage\n\nFirstly, you need to install the package:\n\n```bash\npnpm i llamaindex\n```\n\nSet the OpenAI API key:\n\n```bash\nexport OPENAI_API_KEY=your-api-key\n```\n\nImport the required modules:\n\n```ts\nimport {\n  RelevancyEvaluator,\n  OpenAI,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n```\n\nLet's setup gpt-4 for better results:\n\n```ts\nconst llm = new OpenAI({\n  model: \"gpt-4\",\n});\n\nconst ctx = serviceContextFromDefaults({\n  llm,\n});\n```\n\nNow, let's create a vector index and query engine with documents and query engine respectively. Then, we can evaluate the response with the query and response from the query engine.:\n\n```ts\nconst documents = [\n  new Document({\n    text: `The city came under British control in 1664 and was renamed New York after King Charles II of England granted the lands to his brother, the Duke of York. The city was regained by the Dutch in July 1673 and was renamed New Orange for one year and three months; the city has been continuously named New York since November 1674. New York City was the capital of the United States from 1785 until 1790, and has been the largest U.S. city since 1790. The Statue of Liberty greeted millions of immigrants as they came to the U.S. by ship in the late 19th and early 20th centuries, and is a symbol of the U.S. and its ideals of liberty and peace. In the 21st century, New York City has emerged as a global node of creativity, entrepreneurship, and as a symbol of freedom and cultural diversity. The New York Times has won the most Pulitzer Prizes for journalism and remains the U.S. media's \"newspaper of record\". In 2019, New York City was voted the greatest city in the world in a survey of over 30,000 p...\tPass`,\n  }),\n];\n\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents);\n\nconst queryEngine = vectorIndex.asQueryEngine();\n\nconst query = \"How did New York City get its name?\";\n\nconst response = await queryEngine.query({\n  query,\n});\n\nconst result = await evaluator.evaluateResponse({\n  query,\n  response: response,\n});\n\nconsole.log(`the response is ${result.passing ? \"relevant\" : \"not relevant\"}`);\n```\n\n```bash\nthe response is relevant\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "3466aaa3-47d8-4e56-afdc-adcdb1b868f3": {"__data__": {"id_": "3466aaa3-47d8-4e56-afdc-adcdb1b868f3", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/ingestion_pipeline/index", "file_name": "index.md", "file_size": 2511, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains how to use an `IngestionPipeline` to process input data through various `Transformations`, including a `SimpleNodeParser`, `TitleExtractor`, and `OpenAIEmbedding`. It also demonstrates connecting to a vector database like `QdrantVectorStore` to store and index the processed nodes for future retrieval."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Ingestion Pipeline\n\nAn `IngestionPipeline` uses a concept of `Transformations` that are applied to input data.\nThese `Transformations` are applied to your input data, and the resulting nodes are either returned or inserted into a vector database (if given).\n\n## Usage Pattern\n\nThe simplest usage is to instantiate an IngestionPipeline like so:\n\n```ts\nimport fs from \"node:fs/promises\";\n\nimport {\n  Document,\n  IngestionPipeline,\n  MetadataMode,\n  OpenAIEmbedding,\n  TitleExtractor,\n  SimpleNodeParser,\n} from \"llamaindex\";\n\nasync function main() {\n  // Load essay from abramov.txt in Node\n  const path = \"node_modules/llamaindex/examples/abramov.txt\";\n\n  const essay = await fs.readFile(path, \"utf-8\");\n\n  // Create Document object with essay\n  const document = new Document({ text: essay, id_: path });\n  const pipeline = new IngestionPipeline({\n    transformations: [\n      new SimpleNodeParser({ chunkSize: 1024, chunkOverlap: 20 }),\n      new TitleExtractor(),\n      new OpenAIEmbedding(),\n    ],\n  });\n\n  // run the pipeline\n  const nodes = await pipeline.run({ documents: [document] });\n\n  // print out the result of the pipeline run\n  for (const node of nodes) {\n    console.log(node.getContent(MetadataMode.NONE));\n  }\n}\n\nmain().catch(console.error);\n```\n\n## Connecting to Vector Databases\n\nWhen running an ingestion pipeline, you can also chose to automatically insert the resulting nodes into a remote vector store.\n\nThen, you can construct an index from that vector store later on.\n\n```ts\nimport fs from \"node:fs/promises\";\n\nimport {\n  Document,\n  IngestionPipeline,\n  MetadataMode,\n  OpenAIEmbedding,\n  TitleExtractor,\n  SimpleNodeParser,\n  QdrantVectorStore,\n  VectorStoreIndex,\n} from \"llamaindex\";\n\nasync function main() {\n  // Load essay from abramov.txt in Node\n  const path = \"node_modules/llamaindex/examples/abramov.txt\";\n\n  const essay = await fs.readFile(path, \"utf-8\");\n\n  const vectorStore = new QdrantVectorStore({\n    host: \"http://localhost:6333\",\n  });\n\n  // Create Document object with essay\n  const document = new Document({ text: essay, id_: path });\n  const pipeline = new IngestionPipeline({\n    transformations: [\n      new SimpleNodeParser({ chunkSize: 1024, chunkOverlap: 20 }),\n      new TitleExtractor(),\n      new OpenAIEmbedding(),\n    ],\n    vectorStore,\n  });\n\n  // run the pipeline\n  const nodes = await pipeline.run({ documents: [document] });\n\n  // create an index\n  const index = VectorStoreIndex.fromVectorStore(vectorStore);\n}\n\nmain().catch(console.error);\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "6cf93100-b41e-450f-8fef-720f15fc35c5": {"__data__": {"id_": "6cf93100-b41e-450f-8fef-720f15fc35c5", "embedding": null, "metadata": {"filename": "transformations.md", "extension": ".md", "file_path": "modules/ingestion_pipeline/transformations", "file_name": "transformations.md", "file_size": 2070, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains transformations using Transformation objects like SimpleNodeParser and Embeddings. It demonstrates usage with IngestionPipeline and custom transformations like RemoveSpecialCharacters. The provided code snippets show how to extract metadata and process text data efficiently for node transformation in document processing workflows."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Transformations\n\nA transformation is something that takes a list of nodes as an input, and returns a list of nodes. Each component that implements the Transformatio class has both a `transform` definition responsible for transforming the nodes\n\nCurrently, the following components are Transformation objects:\n\n- SimpleNodeParser\n- MetadataExtractor\n- Embeddings\n\n## Usage Pattern\n\nWhile transformations are best used with with an IngestionPipeline, they can also be used directly.\n\n```ts\nimport { SimpleNodeParser, TitleExtractor, Document } from \"llamaindex\";\n\nasync function main() {\n  let nodes = new SimpleNodeParser().getNodesFromDocuments([\n    new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n  ]);\n\n  const titleExtractor = new TitleExtractor();\n\n  nodes = await titleExtractor.transform(nodes);\n\n  for (const node of nodes) {\n    console.log(node.getContent(MetadataMode.NONE));\n  }\n}\n\nmain().catch(console.error);\n```\n\n## Custom Transformations\n\nYou can implement any transformation yourself by implementing the `TransformerComponent`.\n\nThe following custom transformation will remove any special characters or punctutaion in text.\n\n```ts\nimport { TransformerComponent, Node } from \"llamaindex\";\n\nclass RemoveSpecialCharacters extends TransformerComponent {\n  async transform(nodes: Node[]): Promise<Node[]> {\n    for (const node of nodes) {\n      node.text = node.text.replace(/[^\\w\\s]/gi, \"\");\n    }\n\n    return nodes;\n  }\n}\n```\n\nThese can then be used directly or in any IngestionPipeline.\n\n```ts\nimport { IngestionPipeline, Document } from \"llamaindex\";\n\nasync function main() {\n  const pipeline = new IngestionPipeline({\n    transformations: [new RemoveSpecialCharacters()],\n  });\n\n  const nodes = await pipeline.run({\n    documents: [\n      new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n    ],\n  });\n\n  for (const node of nodes) {\n    console.log(node.getContent(MetadataMode.NONE));\n  }\n}\n\nmain().catch(console.error);\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "4f5d6328-d034-4ace-835f-25e43baf265d": {"__data__": {"id_": "4f5d6328-d034-4ace-835f-25e43baf265d", "embedding": null, "metadata": {"filename": "llamacloud.mdx", "extension": ".mdx", "file_path": "modules/llamacloud", "file_name": "llamacloud.mdx", "file_size": 1597, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains how to utilize existing managed indexes for retrieval in LlamaCloud using LlamaIndexTS. It details the process of creating and using managed indexes, particularly focusing on integration with a chat engine for efficient data retrieval in LLM and RAG applications."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "import CodeBlock from \"@theme/CodeBlock\";\nimport CodeSource from \"!raw-loader!../../../../examples/cloud/chat.ts\";\n\n# LlamaCloud\n\nLlamaCloud is a new generation of managed parsing, ingestion, and retrieval services, designed to bring production-grade context-augmentation to your LLM and RAG applications.\n\nCurrently, LlamaCloud supports\n\n- Managed Ingestion API, handling parsing and document management\n- Managed Retrieval API, configuring optimal retrieval for your RAG system\n\n## Access\n\nWe are opening up a private beta to a limited set of enterprise partners for the managed ingestion and retrieval API. If you\u2019re interested in centralizing your data pipelines and spending more time working on your actual RAG use cases, come talk to us.\n\nIf you have access to LlamaCloud, you can visit LlamaCloud to sign in and get an API key.\n\n## Create a Managed Index\n\nCurrently, you can't create a managed index on LlamaCloud using LlamaIndexTS, but you can use an existing managed index for retrieval that was created by the Python version of LlamaIndex. See the LlamaCloudIndex documentation for more information on how to create a managed index.\n\n## Use a Managed Index\n\nHere's an example of how to use a managed index together with a chat engine:\n\n<CodeBlock language=\"ts\">{CodeSource}</CodeBlock>\n\n## API Reference\n\n- LlamaCloudIndex\n- LlamaCloudRetriever\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "9fc450ec-2b03-478f-b162-282c84941865": {"__data__": {"id_": "9fc450ec-2b03-478f-b162-282c84941865", "embedding": null, "metadata": {"filename": "anthropic.md", "extension": ".md", "file_path": "modules/llms/available_llms/anthropic", "file_name": "anthropic.md", "file_size": 1572, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation covers using Anthropic LLM for document indexing and querying. It demonstrates creating an instance, loading a document, indexing it, setting up a query engine, and executing a query to find the meaning of life. The process involves creating a service context and utilizing VectorStoreIndex and Document classes."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Anthropic\n\n## Usage\n\n```ts\nimport { Anthropic, serviceContextFromDefaults } from \"llamaindex\";\n\nconst anthropicLLM = new Anthropic({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({ llm: anthropicLLM });\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  Anthropic,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Create an instance of the Anthropic LLM\n  const anthropicLLM = new Anthropic({\n    apiKey: \"<YOUR_API_KEY>\",\n  });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({ llm: anthropicLLM });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "25563a44-a4e0-4734-a15d-7f6a94ab369b": {"__data__": {"id_": "25563a44-a4e0-4734-a15d-7f6a94ab369b", "embedding": null, "metadata": {"filename": "azure.md", "extension": ".md", "file_path": "modules/llms/available_llms/azure", "file_name": "azure.md", "file_size": 2056, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation guides users on setting up Azure OpenAI with environment variables, creating an OpenAI instance, indexing documents, querying for information using a query engine, and obtaining responses. It showcases a practical example using GPT-4 model to understand the meaning of life, demonstrating the process from setup to retrieval succinctly."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Azure OpenAI\n\nTo use Azure OpenAI, you only need to set a few environment variables together with the `OpenAI` class.\n\nFor example:\n\n## Environment Variables\n\n```\nexport AZURE_OPENAI_KEY=\"<YOUR KEY HERE>\"\nexport AZURE_OPENAI_ENDPOINT=\"<YOUR ENDPOINT, see https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython&pivots=rest-api>\"\nexport AZURE_OPENAI_DEPLOYMENT=\"gpt-4\" # or some other deployment name\n```\n\n## Usage\n\n```ts\nimport { OpenAI, serviceContextFromDefaults } from \"llamaindex\";\n\nconst azureOpenaiLLM = new OpenAI({ model: \"gpt-4\", temperature: 0 });\n\nconst serviceContext = serviceContextFromDefaults({ llm: azureOpenaiLLM });\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  OpenAI,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Create an instance of the LLM\n  const azureOpenaiLLM = new OpenAI({ model: \"gpt-4\", temperature: 0 });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({ llm: azureOpenaiLLM });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "bcac6100-0e77-4fc3-9ed8-822567657870": {"__data__": {"id_": "bcac6100-0e77-4fc3-9ed8-822567657870", "embedding": null, "metadata": {"filename": "fireworks.md", "extension": ".md", "file_path": "modules/llms/available_llms/fireworks", "file_name": "fireworks.md", "file_size": 1562, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation page provides guidance on using Fireworks.ai's LLM for production tasks, focusing on speed and quality. It demonstrates loading a PDF, creating embeddings, and querying the index for specific information, like mistakes made by Warren E. Buffett in the Berkshire Hathaway 2022 annual report."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Fireworks LLM\n\nFireworks.ai focus on production use cases for open source LLMs, offering speed and quality.\n\n## Usage\n\n```ts\nimport { FireworksLLM, serviceContextFromDefaults } from \"llamaindex\";\n\nconst fireworksLLM = new FireworksLLM({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({ llm: fireworksLLM });\n```\n\n## Load and index documents\n\nFor this example, we will load the Berkshire Hathaway 2022 annual report pdf\n\n```ts\nconst reader = new PDFReader();\nconst documents = await reader.loadData(\"../data/brk-2022.pdf\");\n\n// Split text and create embeddings. Store them in a VectorStoreIndex\nconst index = await VectorStoreIndex.fromDocuments(documents, {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({\n  query: \"What mistakes did Warren E. Buffett make?\",\n});\n```\n\n## Full Example\n\n```ts\nimport { VectorStoreIndex } from \"llamaindex\";\nimport { PDFReader } from \"llamaindex/readers/PDFReader\";\n\nasync function main() {\n  // Load PDF\n  const reader = new PDFReader();\n  const documents = await reader.loadData(\"../data/brk-2022.pdf\");\n\n  // Split text and create embeddings. Store them in a VectorStoreIndex\n  const index = await VectorStoreIndex.fromDocuments(documents);\n\n  // Query the index\n  const queryEngine = index.asQueryEngine();\n  const response = await queryEngine.query({\n    query: \"What mistakes did Warren E. Buffett make?\",\n  });\n\n  // Output response\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "3ad25922-dde9-4b1d-b549-8a069f71e533": {"__data__": {"id_": "3ad25922-dde9-4b1d-b549-8a069f71e533", "embedding": null, "metadata": {"filename": "groq.mdx", "extension": ".mdx", "file_path": "modules/llms/available_llms/groq", "file_name": "groq.mdx", "file_size": 1230, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation covers using Groq with LlamaIndex. It explains setting up an API key, initializing Groq, loading and indexing documents, and querying. The example demonstrates creating a document, indexing it, and querying for information. The page provides a comprehensive guide on integrating Groq for document indexing and retrieval."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "import CodeBlock from \"@theme/CodeBlock\";\nimport CodeSource from \"!raw-loader!../../../../../../examples/groq.ts\";\n\n# Groq\n\n## Usage\n\nFirst, create an API key at the Groq Console. Then save it in your environment:\n\n```bash\nexport GROQ_API_KEY=<your-api-key>\n```\n\nThe initialize the Groq module.\n\n```ts\nimport { Groq, serviceContextFromDefaults } from \"llamaindex\";\n\nconst groq = new Groq({\n  // If you do not wish to set your API key in the environment, you may\n  // configure your API key when you initialize the Groq class.\n  // apiKey: \"<your-api-key>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({ llm: groq });\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n<CodeBlock language=\"ts\" showLineNumbers>\n  {CodeSource}\n</CodeBlock>\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "3c9685c2-39f8-46ad-aa17-c11605ed2ca6": {"__data__": {"id_": "3c9685c2-39f8-46ad-aa17-c11605ed2ca6", "embedding": null, "metadata": {"filename": "llama2.md", "extension": ".md", "file_path": "modules/llms/available_llms/llama2", "file_name": "llama2.md", "file_size": 1992, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation page provides guidance on using LlamaIndex for document indexing and querying. It demonstrates creating a LlamaDeuce instance, loading a document, indexing it, and querying for information. The process involves setting up a service context, replicating sessions, and utilizing query engines for efficient retrieval of information."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# LLama2\n\n## Usage\n\n```ts\nimport { Ollama, serviceContextFromDefaults } from \"llamaindex\";\n\nconst llama2LLM = new LlamaDeuce({ chatStrategy: DeuceChatStrategy.META });\n\nconst serviceContext = serviceContextFromDefaults({ llm: llama2LLM });\n```\n\n## Usage with Replication\n\n```ts\nimport {\n  Ollama,\n  ReplicateSession,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nconst replicateSession = new ReplicateSession({\n  replicateKey,\n});\n\nconst llama2LLM = new LlamaDeuce({\n  chatStrategy: DeuceChatStrategy.META,\n  replicateSession,\n});\n\nconst serviceContext = serviceContextFromDefaults({ llm: llama2LLM });\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  LlamaDeuce,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Create an instance of the LLM\n  const llama2LLM = new LlamaDeuce({ chatStrategy: DeuceChatStrategy.META });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({ llm: mistralLLM });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "00e889f4-6482-4e88-a8c5-dacbea93cb9e": {"__data__": {"id_": "00e889f4-6482-4e88-a8c5-dacbea93cb9e", "embedding": null, "metadata": {"filename": "mistral.md", "extension": ".md", "file_path": "modules/llms/available_llms/mistral", "file_name": "mistral.md", "file_size": 1625, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation covers using MistralAI from LlamaIndex to load, index, and query documents. It demonstrates creating a service context, indexing a document, setting up a query engine, and executing a query to find the meaning of life. The example showcases the process of leveraging MistralAI for document retrieval and analysis."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Mistral\n\n## Usage\n\n```ts\nimport { Ollama, serviceContextFromDefaults } from \"llamaindex\";\n\nconst mistralLLM = new MistralAI({\n  model: \"mistral-tiny\",\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({ llm: mistralLLM });\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  MistralAI,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Create an instance of the LLM\n  const mistralLLM = new MistralAI({ model: \"mistral-tiny\" });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({ llm: mistralLLM });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "f04b9979-e6a0-4646-b1b7-68412e205589": {"__data__": {"id_": "f04b9979-e6a0-4646-b1b7-68412e205589", "embedding": null, "metadata": {"filename": "ollama.md", "extension": ".md", "file_path": "modules/llms/available_llms/ollama", "file_name": "ollama.md", "file_size": 1836, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation covers using Ollama for document indexing and querying. It demonstrates creating an Ollama instance, loading a document, indexing it, and querying with a specific question. The process involves setting up service context, utilizing VectorStoreIndex, and querying for relevant information using the LlamaIndex package."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Ollama\n\n## Usage\n\n```ts\nimport { Ollama, serviceContextFromDefaults } from \"llamaindex\";\n\nconst ollamaLLM = new Ollama({ model: \"llama2\", temperature: 0.75 });\n\nconst serviceContext = serviceContextFromDefaults({\n  llm: ollamaLLM,\n  embedModel: ollamaLLM,\n});\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  Ollama,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nimport fs from \"fs/promises\";\n\nasync function main() {\n  // Create an instance of the LLM\n  const ollamaLLM = new Ollama({ model: \"llama2\", temperature: 0.75 });\n\n  const essay = await fs.readFile(\"./paul_graham_essay.txt\", \"utf-8\");\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({\n    embedModel: ollamaLLM, // prevent 'Set OpenAI Key in OPENAI_API_KEY env variable' error\n    llm: ollamaLLM,\n  });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "78e45fa1-2304-4ec1-a5d9-2524c3bc1fcb": {"__data__": {"id_": "78e45fa1-2304-4ec1-a5d9-2524c3bc1fcb", "embedding": null, "metadata": {"filename": "openai.md", "extension": ".md", "file_path": "modules/llms/available_llms/openai", "file_name": "openai.md", "file_size": 1742, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation guides users on setting up an OpenAI model, indexing a document, and querying it using LlamaIndex. It showcases creating a service context, loading a single document, and querying for information like the meaning of life. The example demonstrates how to utilize LlamaIndex for document retrieval and querying efficiently."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# OpenAI\n\n```ts\nimport { OpenAI, serviceContextFromDefaults } from \"llamaindex\";\n\nconst openaiLLM = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0, apiKey: <YOUR_API_KEY> });\n\nconst serviceContext = serviceContextFromDefaults({ llm: openaiLLM });\n```\n\nYou can setup the apiKey on the environment variables, like:\n\n```bash\nexport OPENAI_API_KEY=\"<YOUR_API_KEY>\"\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  OpenAI,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Create an instance of the LLM\n  const openaiLLM = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({ llm: openaiLLM });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "a1d0e3e1-17f5-43ee-9278-04ea6251b57c": {"__data__": {"id_": "a1d0e3e1-17f5-43ee-9278-04ea6251b57c", "embedding": null, "metadata": {"filename": "portkey.md", "extension": ".md", "file_path": "modules/llms/available_llms/portkey", "file_name": "portkey.md", "file_size": 1609, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation provides guidance on using Portkey LLM for document indexing and querying. It demonstrates creating a service context, loading and indexing documents, and executing queries to retrieve relevant information. The example showcases integrating Portkey, Document, VectorStoreIndex, and query engines to facilitate efficient information retrieval processes."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Portkey LLM\n\n## Usage\n\n```ts\nimport { Portkey, serviceContextFromDefaults } from \"llamaindex\";\n\nconst portkeyLLM = new Portkey({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({ llm: portkeyLLM });\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  Portkey,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Create an instance of the LLM\n  const portkeyLLM = new Portkey({\n    apiKey: \"<YOUR_API_KEY>\",\n  });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({ llm: portkeyLLM });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "48787b87-935d-4639-ac1d-e9a17ce1b778": {"__data__": {"id_": "48787b87-935d-4639-ac1d-e9a17ce1b778", "embedding": null, "metadata": {"filename": "together.md", "extension": ".md", "file_path": "modules/llms/available_llms/together", "file_name": "together.md", "file_size": 1630, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation provides a guide on using TogetherLLM for document indexing and querying. It includes setting up the LLM instance, creating a service context, loading and indexing documents, querying for information, and a full example demonstrating the process. The focus is on leveraging TogetherLLM for efficient document retrieval and analysis."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Together LLM\n\n## Usage\n\n```ts\nimport { TogetherLLM, serviceContextFromDefaults } from \"llamaindex\";\n\nconst togetherLLM = new TogetherLLM({\n  apiKey: \"<YOUR_API_KEY>\",\n});\n\nconst serviceContext = serviceContextFromDefaults({ llm: togetherLLM });\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Query\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst query = \"What is the meaning of life?\";\n\nconst results = await queryEngine.query({\n  query,\n});\n```\n\n## Full Example\n\n```ts\nimport {\n  TogetherLLM,\n  Document,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Create an instance of the LLM\n  const togetherLLM = new TogetherLLM({\n    apiKey: \"<YOUR_API_KEY>\",\n  });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({ llm: togetherLLM });\n\n  const document = new Document({ text: essay, id_: \"essay\" });\n\n  // Load and index documents\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    serviceContext,\n  });\n\n  // get retriever\n  const retriever = index.asRetriever();\n\n  // Create a query engine\n  const queryEngine = index.asQueryEngine({\n    retriever,\n  });\n\n  const query = \"What is the meaning of life?\";\n\n  // Query\n  const response = await queryEngine.query({\n    query,\n  });\n\n  // Log the response\n  console.log(response.response);\n}\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "ff8eecf2-bc51-4151-88e5-7861718b6b5d": {"__data__": {"id_": "ff8eecf2-bc51-4151-88e5-7861718b6b5d", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/llms/index", "file_name": "index.md", "file_size": 1107, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page covers the use of Large Language Models (LLMs) like `gpt-3.5-turbo` for generating natural language responses. It explains setting LLMs in the `ServiceContext` object, using Azure OpenAI with environment variables, recommending Ollama LLM for local use, and lists API references for OpenAI and ServiceContext."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 3\n---\n\n# Large Language Models (LLMs)\n\nThe LLM is responsible for reading text and generating natural language responses to queries. By default, LlamaIndex.TS uses `gpt-3.5-turbo`.\n\nThe LLM can be explicitly set in the `ServiceContext` object.\n\n```typescript\nimport { OpenAI, serviceContextFromDefaults } from \"llamaindex\";\n\nconst openaiLLM = new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\n\nconst serviceContext = serviceContextFromDefaults({ llm: openaiLLM });\n```\n\n## Azure OpenAI\n\nTo use Azure OpenAI, you only need to set a few environment variables.\n\nFor example:\n\n```\nexport AZURE_OPENAI_KEY=\"<YOUR KEY HERE>\"\nexport AZURE_OPENAI_ENDPOINT=\"<YOUR ENDPOINT, see https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart?tabs=command-line%2Cpython&pivots=rest-api>\"\nexport AZURE_OPENAI_DEPLOYMENT=\"gpt-4\" # or some other deployment name\n```\n\n## Local LLM\n\nFor local LLMs, currently we recommend the use of Ollama LLM.\n\n## API Reference\n\n- OpenAI\n- ServiceContext\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "2980ff31-8d0d-42a6-b022-3cda447db084": {"__data__": {"id_": "2980ff31-8d0d-42a6-b022-3cda447db084", "embedding": null, "metadata": {"filename": "node_parser.md", "extension": ".md", "file_path": "modules/node_parser", "file_name": "node_parser.md", "file_size": 2733, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page covers the functionality of NodeParser for splitting documents into nodes, TextSplitter for sentence splitting, and MarkdownNodeParser for handling markdown documents. It demonstrates usage examples and provides output metadata. The page also references SimpleNodeParser and SentenceSplitter in the API Reference section."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 4\n---\n\n# NodeParser\n\nThe `NodeParser` in LlamaIndex is responsible for splitting `Document` objects into more manageable `Node` objects. When you call `.fromDocuments()`, the `NodeParser` from the `ServiceContext` is used to do this automatically for you. Alternatively, you can use it to split documents ahead of time.\n\n```typescript\nimport { Document, SimpleNodeParser } from \"llamaindex\";\n\nconst nodeParser = new SimpleNodeParser();\nconst nodes = nodeParser.getNodesFromDocuments([\n  new Document({ text: \"I am 10 years old. John is 20 years old.\" }),\n]);\n```\n\n## TextSplitter\n\nThe underlying text splitter will split text by sentences. It can also be used as a standalone module for splitting raw text.\n\n```typescript\nimport { SentenceSplitter } from \"llamaindex\";\n\nconst splitter = new SentenceSplitter({ chunkSize: 1 });\n\nconst textSplits = splitter.splitText(\"Hello World\");\n```\n\n## MarkdownNodeParser\n\nThe `MarkdownNodeParser` is a more advanced `NodeParser` that can handle markdown documents. It will split the markdown into nodes and then parse the nodes into a `Document` object.\n\n```typescript\nimport { MarkdownNodeParser } from \"llamaindex\";\n\nconst nodeParser = new MarkdownNodeParser();\n\nconst nodes = nodeParser.getNodesFromDocuments([\n  new Document({\n    text: `# Main Header\nMain content\n\n# Header 2\nHeader 2 content\n\n## Sub-header\nSub-header content\n\n  `,\n  }),\n]);\n```\n\nThe output metadata will be something like:\n\n```bash\n[\n  TextNode {\n    id_: '008e41a8-b097-487c-bee8-bd88b9455844',\n    metadata: { 'Header 1': 'Main Header' },\n    excludedEmbedMetadataKeys: [],\n    excludedLlmMetadataKeys: [],\n    relationships: { PARENT: [Array] },\n    hash: 'KJ5e/um/RkHaNR6bonj9ormtZY7I8i4XBPVYHXv1A5M=',\n    text: 'Main Header\\nMain content',\n    textTemplate: '',\n    metadataSeparator: '\\n'\n  },\n  TextNode {\n    id_: '0f5679b3-ba63-4aff-aedc-830c4208d0b5',\n    metadata: { 'Header 1': 'Header 2' },\n    excludedEmbedMetadataKeys: [],\n    excludedLlmMetadataKeys: [],\n    relationships: { PARENT: [Array] },\n    hash: 'IP/g/dIld3DcbK+uHzDpyeZ9IdOXY4brxhOIe7wc488=',\n    text: 'Header 2\\nHeader 2 content',\n    textTemplate: '',\n    metadataSeparator: '\\n'\n  },\n  TextNode {\n    id_: 'e81e9bd0-121c-4ead-8ca7-1639d65fdf90',\n    metadata: { 'Header 1': 'Header 2', 'Header 2': 'Sub-header' },\n    excludedEmbedMetadataKeys: [],\n    excludedLlmMetadataKeys: [],\n    relationships: { PARENT: [Array] },\n    hash: 'B3kYNnxaYi9ghtAgwza0ZEVKF4MozobkNUlcekDL7JQ=',\n    text: 'Sub-header\\nSub-header content',\n    textTemplate: '',\n    metadataSeparator: '\\n'\n  }\n]\n```\n\n## API Reference\n\n- SimpleNodeParser\n- SentenceSplitter\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "44e1fac8-661c-43be-bbc9-6505eeb04fed": {"__data__": {"id_": "44e1fac8-661c-43be-bbc9-6505eeb04fed", "embedding": null, "metadata": {"filename": "cohere_reranker.md", "extension": ".md", "file_path": "modules/node_postprocessors/cohere_reranker", "file_name": "cohere_reranker.md", "file_size": 1876, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation explains how to use the Cohere Reranker with the Cohere API to enhance search results. It covers installing the required package, setting up the API key, loading and indexing documents, adjusting result retrieval settings, creating the CohereRerank instance with the API key, and utilizing the query engine for improved search outcomes."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Cohere Reranker\n\nThe Cohere Reranker is a postprocessor that uses the Cohere API to rerank the results of a search query.\n\n## Setup\n\nFirstly, you will need to install the `llamaindex` package.\n\n```bash\npnpm install llamaindex\n```\n\nNow, you will need to sign up for an API key at Cohere. Once you have your API key you can import the necessary modules and create a new instance of the `CohereRerank` class.\n\n```ts\nimport {\n  CohereRerank,\n  Document,\n  OpenAI,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n```\n\n## Load and index documents\n\nFor this example, we will use a single document. In a real-world scenario, you would have multiple documents to index.\n\n```ts\nconst document = new Document({ text: essay, id_: \"essay\" });\n\nconst serviceContext = serviceContextFromDefaults({\n  llm: new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0.1 }),\n});\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n```\n\n## Increase similarity topK to retrieve more results\n\nThe default value for `similarityTopK` is 2. This means that only the most similar document will be returned. To retrieve more results, you can increase the value of `similarityTopK`.\n\n```ts\nconst retriever = index.asRetriever();\nretriever.similarityTopK = 5;\n```\n\n## Create a new instance of the CohereRerank class\n\nThen you can create a new instance of the `CohereRerank` class and pass in your API key and the number of results you want to return.\n\n```ts\nconst nodePostprocessor = new CohereRerank({\n  apiKey: \"<COHERE_API_KEY>\",\n  topN: 4,\n});\n```\n\n## Create a query engine with the retriever and node postprocessor\n\n```ts\nconst queryEngine = index.asQueryEngine({\n  retriever,\n  nodePostprocessors: [nodePostprocessor],\n});\n\n// log the response\nconst response = await queryEngine.query(\"Where did the author grown up?\");\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "83ed8fd4-28da-4a3a-90e0-441ba17a4874": {"__data__": {"id_": "83ed8fd4-28da-4a3a-90e0-441ba17a4874", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/node_postprocessors/index", "file_name": "index.md", "file_size": 2888, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "Node postprocessors in LlamaIndex are modules that transform or filter nodes after retrieval but before response synthesis. Users can apply built-in or custom postprocessors. Examples include filtering nodes based on similarity scores and reranking nodes using a trained model. These postprocessors are commonly used in query engines for node processing."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Node Postprocessors\n\n## Concept\n\nNode postprocessors are a set of modules that take a set of nodes, and apply some kind of transformation or filtering before returning them.\n\nIn LlamaIndex, node postprocessors are most commonly applied within a query engine, after the node retrieval step and before the response synthesis step.\n\nLlamaIndex offers several node postprocessors for immediate use, while also providing a simple API for adding your own custom postprocessors.\n\n## Usage Pattern\n\nAn example of using a node postprocessors is below:\n\n```ts\nimport {\n  Node,\n  NodeWithScore,\n  SimilarityPostprocessor,\n  CohereRerank,\n} from \"llamaindex\";\n\nconst nodes: NodeWithScore[] = [\n  {\n    node: new TextNode({ text: \"hello world\" }),\n    score: 0.8,\n  },\n  {\n    node: new TextNode({ text: \"LlamaIndex is the best\" }),\n    score: 0.6,\n  },\n];\n\n// similarity postprocessor: filter nodes below 0.75 similarity score\nconst processor = new SimilarityPostprocessor({\n  similarityCutoff: 0.7,\n});\n\nconst filteredNodes = processor.postprocessNodes(nodes);\n\n// cohere rerank: rerank nodes given query using trained model\nconst reranker = new CohereRerank({\n  apiKey: \"<COHERE_API_KEY>\",\n  topN: 2,\n});\n\nconst rerankedNodes = await reranker.postprocessNodes(nodes, \"<user_query>\");\n\nconsole.log(filteredNodes, rerankedNodes);\n```\n\nNow you can use the `filteredNodes` and `rerankedNodes` in your application.\n\n## Using Node Postprocessors in LlamaIndex\n\nMost commonly, node-postprocessors will be used in a query engine, where they are applied to the nodes returned from a retriever, and before the response synthesis step.\n\n### Using Node Postprocessors in a Query Engine\n\n```ts\nimport { Node, NodeWithScore, SimilarityPostprocessor, CohereRerank } from \"llamaindex\";\n\nconst nodes: NodeWithScore[] = [\n  {\n    node: new TextNode({ text: \"hello world\" }),\n    score: 0.8,\n  },\n  {\n    node: new TextNode({ text: \"LlamaIndex is the best\" }),\n    score: 0.6,\n  }\n];\n\n// cohere rerank: rerank nodes given query using trained model\nconst reranker = new CohereRerank({\n  apiKey: \"<COHERE_API_KEY>,\n  topN: 2,\n})\n\nconst document = new Document({ text: \"essay\", id_: \"essay\" });\n\nconst serviceContext = serviceContextFromDefaults({\n  llm: new OpenAI({ model: \"gpt-3.5-turbo\", temperature: 0.1 }),\n});\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\nconst queryEngine = index.asQueryEngine({\n  nodePostprocessors: [processor, reranker],\n});\n\n// all node post-processors will be applied during each query\nconst response = await queryEngine.query(\"<user_query>\");\n```\n\n### Using with retrieved nodes\n\n```ts\nimport { SimilarityPostprocessor } from \"llamaindex\";\n\nnodes = await index.asRetriever().retrieve(\"test query str\");\n\nconst processor = new SimilarityPostprocessor({\n  similarityCutoff: 0.7,\n});\n\nconst filteredNodes = processor.postprocessNodes(nodes);\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "e08436f6-0cae-4ac6-ac76-08897f3cbba0": {"__data__": {"id_": "e08436f6-0cae-4ac6-ac76-08897f3cbba0", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/prompt/index", "file_name": "index.md", "file_size": 2717, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation explains how to customize prompts using two methods: customizing the default prompt on initialization and customizing submodules prompt. Users can create their own prompt templates to tailor the behavior of the framework. By overriding the default prompt, users can enhance the expressive power of LlamaIndex through personalized prompts."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Prompts\n\nPrompting is the fundamental input that gives LLMs their expressive power. LlamaIndex uses prompts to build the index, do insertion, perform traversal during querying, and to synthesize the final answer.\n\nUsers may also provide their own prompt templates to further customize the behavior of the framework. The best method for customizing is copying the default prompt from the link above, and using that as the base for any modifications.\n\n## Usage Pattern\n\nCurrently, there are two ways to customize prompts in LlamaIndex:\n\nFor both methods, you will need to create an function that overrides the default prompt.\n\n```ts\n// Define a custom prompt\nconst newTextQaPrompt: TextQaPrompt = ({ context, query }) => {\n  return `Context information is below.\n---------------------\n${context}\n---------------------\nGiven the context information and not prior knowledge, answer the query.\nAnswer the query in the style of a Sherlock Holmes detective novel.\nQuery: ${query}\nAnswer:`;\n};\n```\n\n### 1. Customizing the default prompt on initialization\n\nThe first method is to create a new instance of `ResponseSynthesizer` (or the module you would like to update the prompt) and pass the custom prompt to the `responseBuilder` parameter. Then, pass the instance to the `asQueryEngine` method of the index.\n\n```ts\n// Create an instance of response synthesizer\nconst responseSynthesizer = new ResponseSynthesizer({\n  responseBuilder: new CompactAndRefine(serviceContext, newTextQaPrompt),\n});\n\n// Create index\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\n// Query the index\nconst queryEngine = index.asQueryEngine({ responseSynthesizer });\n\nconst response = await queryEngine.query({\n  query: \"What did the author do in college?\",\n});\n```\n\n### 2. Customizing submodules prompt\n\nThe second method is that most of the modules in LlamaIndex have a `getPrompts` and a `updatePrompt` method that allows you to override the default prompt. This method is useful when you want to change the prompt on the fly or in submodules on a more granular level.\n\n```ts\n// Create index\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  serviceContext,\n});\n\n// Query the index\nconst queryEngine = index.asQueryEngine();\n\n// Get a list of prompts for the query engine\nconst prompts = queryEngine.getPrompts();\n\n// output: { \"responseSynthesizer:textQATemplate\": defaultTextQaPrompt, \"responseSynthesizer:refineTemplate\": defaultRefineTemplatePrompt }\n\n// Now, we can override the default prompt\nqueryEngine.updatePrompt({\n  \"responseSynthesizer:textQATemplate\": newTextQaPrompt,\n});\n\nconst response = await queryEngine.query({\n  query: \"What did the author do in college?\",\n});\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "ff0d576b-cb6a-4b1b-8e2f-68b58d4d3e81": {"__data__": {"id_": "ff0d576b-cb6a-4b1b-8e2f-68b58d4d3e81", "embedding": null, "metadata": {"filename": "index.md", "extension": ".md", "file_path": "modules/query_engines/index", "file_name": "index.md", "file_size": 2165, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page provides insights into the Tools concept, focusing on QueryEngineTool for running queries on a QueryEngine. It introduces the Sub Question Query Engine, which breaks down queries into multiple parts to generate a cohesive response. The page also mentions the ability to stream responses from the QueryEngine."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# QueryEngine\n\nA query engine wraps a `Retriever` and a `ResponseSynthesizer` into a pipeline, that will use the query string to fetech nodes and then send them to the LLM to generate a response.\n\n```typescript\nconst queryEngine = index.asQueryEngine();\nconst response = await queryEngine.query({ query: \"query string\" });\n```\n\nThe `query` function also supports streaming, just add `stream: true` as an option:\n\n```typescript\nconst stream = await queryEngine.query({ query: \"query string\", stream: true });\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.response);\n}\n```\n\n## Sub Question Query Engine\n\nThe basic concept of the Sub Question Query Engine is that it splits a single query into multiple queries, gets an answer for each of those queries, and then combines those different answers into a single coherent response for the user. You can think of it as the \"think this through step by step\" prompt technique but iterating over your data sources!\n\n### Getting Started\n\nThe easiest way to start trying the Sub Question Query Engine is running the subquestion.ts file in examples.\n\n```bash\nnpx ts-node subquestion.ts\n```\n\n### Tools\n\nSubQuestionQueryEngine is implemented with Tools. The basic idea of Tools is that they are executable options for the large language model. In this case, our SubQuestionQueryEngine relies on QueryEngineTool, which as you guessed it is a tool to run queries on a QueryEngine. This allows us to give the model an option to query different documents for different questions for example. You could also imagine that the SubQuestionQueryEngine could use a Tool that searches for something on the web or gets an answer using Wolfram Alpha.\n\nYou can learn more about Tools by taking a look at the LlamaIndex Python documentation https://gpt-index.readthedocs.io/en/latest/core_modules/agent_modules/tools/root.html\n\n## API Reference\n\n- RetrieverQueryEngine\n- SubQuestionQueryEngine\n- QueryEngineTool\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "b7ff3d30-46c4-4a25-831a-11aef9044a70": {"__data__": {"id_": "b7ff3d30-46c4-4a25-831a-11aef9044a70", "embedding": null, "metadata": {"filename": "metadata_filtering.md", "extension": ".md", "file_path": "modules/query_engines/metadata_filtering", "file_name": "metadata_filtering.md", "file_size": 3403, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains metadata filtering for document retrieval. It covers setting up the `llamaindex` package, creating documents with metadata, establishing a `ChromaVectorStore`, and querying the index with metadata filters. The example showcases filtering documents by metadata like dog color and ID for targeted search results."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Metadata Filtering\n\nMetadata filtering is a way to filter the documents that are returned by a query based on the metadata associated with the documents. This is useful when you want to filter the documents based on some metadata that is not part of the document text.\n\nYou can also check our multi-tenancy blog post to see how metadata filtering can be used in a multi-tenant environment. [https://blog.llamaindex.ai/building-multi-tenancy-rag-system-with-llamaindex-0d6ab4e0c44b] (the article uses the Python version of LlamaIndex, but the concepts are the same).\n\n## Setup\n\nFirstly if you haven't already, you need to install the `llamaindex` package:\n\n```bash\npnpm i llamaindex\n```\n\nThen you can import the necessary modules from `llamaindex`:\n\n```ts\nimport {\n  ChromaVectorStore,\n  Document,\n  VectorStoreIndex,\n  storageContextFromDefaults,\n} from \"llamaindex\";\n\nconst collectionName = \"dog_colors\";\n```\n\n## Creating documents with metadata\n\nYou can create documents with metadata using the `Document` class:\n\n```ts\nconst docs = [\n  new Document({\n    text: \"The dog is brown\",\n    metadata: {\n      color: \"brown\",\n      dogId: \"1\",\n    },\n  }),\n  new Document({\n    text: \"The dog is red\",\n    metadata: {\n      color: \"red\",\n      dogId: \"2\",\n    },\n  }),\n];\n```\n\n## Creating a ChromaDB vector store\n\nYou can create a `ChromaVectorStore` to store the documents:\n\n```ts\nconst chromaVS = new ChromaVectorStore({ collectionName });\nconst serviceContext = await storageContextFromDefaults({\n  vectorStore: chromaVS,\n});\n\nconst index = await VectorStoreIndex.fromDocuments(docs, {\n  storageContext: serviceContext,\n});\n```\n\n## Querying the index with metadata filtering\n\nNow you can query the index with metadata filtering using the `preFilters` option:\n\n```ts\nconst queryEngine = index.asQueryEngine({\n  preFilters: {\n    filters: [\n      {\n        key: \"dogId\",\n        value: \"2\",\n        filterType: \"ExactMatch\",\n      },\n    ],\n  },\n});\n\nconst response = await queryEngine.query({\n  query: \"What is the color of the dog?\",\n});\n\nconsole.log(response.toString());\n```\n\n## Full Code\n\n```ts\nimport {\n  ChromaVectorStore,\n  Document,\n  VectorStoreIndex,\n  storageContextFromDefaults,\n} from \"llamaindex\";\n\nconst collectionName = \"dog_colors\";\n\nasync function main() {\n  try {\n    const docs = [\n      new Document({\n        text: \"The dog is brown\",\n        metadata: {\n          color: \"brown\",\n          dogId: \"1\",\n        },\n      }),\n      new Document({\n        text: \"The dog is red\",\n        metadata: {\n          color: \"red\",\n          dogId: \"2\",\n        },\n      }),\n    ];\n\n    console.log(\"Creating ChromaDB vector store\");\n    const chromaVS = new ChromaVectorStore({ collectionName });\n    const ctx = await storageContextFromDefaults({ vectorStore: chromaVS });\n\n    console.log(\"Embedding documents and adding to index\");\n    const index = await VectorStoreIndex.fromDocuments(docs, {\n      storageContext: ctx,\n    });\n\n    console.log(\"Querying index\");\n    const queryEngine = index.asQueryEngine({\n      preFilters: {\n        filters: [\n          {\n            key: \"dogId\",\n            value: \"2\",\n            filterType: \"ExactMatch\",\n          },\n        ],\n      },\n    });\n    const response = await queryEngine.query({\n      query: \"What is the color of the dog?\",\n    });\n    console.log(response.toString());\n  } catch (e) {\n    console.error(e);\n  }\n}\n\nmain();\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "770009bf-4432-4d86-b50b-e13fb47ba24a": {"__data__": {"id_": "770009bf-4432-4d86-b50b-e13fb47ba24a", "embedding": null, "metadata": {"filename": "router_query_engine.md", "extension": ".md", "file_path": "modules/query_engines/router_query_engine", "file_name": "router_query_engine.md", "file_size": 4662, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "Learn to create a custom router query engine in LlamaIndex to select between different query engines for document parsing and indexing. Understand loading data, defining service context, creating indices like VectorStoreIndex and SummaryIndex, building query engines, and utilizing a RouterQueryEngine to handle queries effectively based on specific needs."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Router Query Engine\n\nIn this tutorial, we define a custom router query engine that selects one out of several candidate query engines to execute a query.\n\n## Setup\n\nFirst, we need to install import the necessary modules from `llamaindex`:\n\n```bash\npnpm i lamaindex\n```\n\n```ts\nimport {\n  OpenAI,\n  RouterQueryEngine,\n  SimpleDirectoryReader,\n  SimpleNodeParser,\n  SummaryIndex,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n```\n\n## Loading Data\n\nNext, we need to load some data. We will use the `SimpleDirectoryReader` to load documents from a directory:\n\n```ts\nconst documents = await new SimpleDirectoryReader().loadData({\n  directoryPath: \"node_modules/llamaindex/examples\",\n});\n```\n\n## Service Context\n\nNext, we need to define some basic rules and parse the documents into nodes. We will use the `SimpleNodeParser` to parse the documents into nodes and `ServiceContext` to define the rules (eg. LLM API key, chunk size, etc.):\n\n```ts\nconst nodeParser = new SimpleNodeParser({\n  chunkSize: 1024,\n});\n\nconst serviceContext = serviceContextFromDefaults({\n  nodeParser,\n  llm: new OpenAI(),\n});\n```\n\n## Creating Indices\n\nNext, we need to create some indices. We will create a `VectorStoreIndex` and a `SummaryIndex`:\n\n```ts\nconst vectorIndex = await VectorStoreIndex.fromDocuments(documents, {\n  serviceContext,\n});\n\nconst summaryIndex = await SummaryIndex.fromDocuments(documents, {\n  serviceContext,\n});\n```\n\n## Creating Query Engines\n\nNext, we need to create some query engines. We will create a `VectorStoreQueryEngine` and a `SummaryQueryEngine`:\n\n```ts\nconst vectorQueryEngine = vectorIndex.asQueryEngine();\nconst summaryQueryEngine = summaryIndex.asQueryEngine();\n```\n\n## Creating a Router Query Engine\n\nNext, we need to create a router query engine. We will use the `RouterQueryEngine` to create a router query engine:\n\nWe're defining two query engines, one for summarization and one for retrieving specific context. The router query engine will select the most appropriate query engine based on the query.\n\n```ts\nconst queryEngine = RouterQueryEngine.fromDefaults({\n  queryEngineTools: [\n    {\n      queryEngine: vectorQueryEngine,\n      description: \"Useful for summarization questions related to Abramov\",\n    },\n    {\n      queryEngine: summaryQueryEngine,\n      description: \"Useful for retrieving specific context from Abramov\",\n    },\n  ],\n  serviceContext,\n});\n```\n\n## Querying the Router Query Engine\n\nFinally, we can query the router query engine:\n\n```ts\nconst summaryResponse = await queryEngine.query({\n  query: \"Give me a summary about his past experiences?\",\n});\n\nconsole.log({\n  answer: summaryResponse.response,\n  metadata: summaryResponse?.metadata?.selectorResult,\n});\n```\n\n## Full code\n\n```ts\nimport {\n  OpenAI,\n  RouterQueryEngine,\n  SimpleDirectoryReader,\n  SimpleNodeParser,\n  SummaryIndex,\n  VectorStoreIndex,\n  serviceContextFromDefaults,\n} from \"llamaindex\";\n\nasync function main() {\n  // Load documents from a directory\n  const documents = await new SimpleDirectoryReader().loadData({\n    directoryPath: \"node_modules/llamaindex/examples\",\n  });\n\n  // Parse the documents into nodes\n  const nodeParser = new SimpleNodeParser({\n    chunkSize: 1024,\n  });\n\n  // Create a service context\n  const serviceContext = serviceContextFromDefaults({\n    nodeParser,\n    llm: new OpenAI(),\n  });\n\n  // Create indices\n  const vectorIndex = await VectorStoreIndex.fromDocuments(documents, {\n    serviceContext,\n  });\n\n  const summaryIndex = await SummaryIndex.fromDocuments(documents, {\n    serviceContext,\n  });\n\n  // Create query engines\n  const vectorQueryEngine = vectorIndex.asQueryEngine();\n  const summaryQueryEngine = summaryIndex.asQueryEngine();\n\n  // Create a router query engine\n  const queryEngine = RouterQueryEngine.fromDefaults({\n    queryEngineTools: [\n      {\n        queryEngine: vectorQueryEngine,\n        description: \"Useful for summarization questions related to Abramov\",\n      },\n      {\n        queryEngine: summaryQueryEngine,\n        description: \"Useful for retrieving specific context from Abramov\",\n      },\n    ],\n    serviceContext,\n  });\n\n  // Query the router query engine\n  const summaryResponse = await queryEngine.query({\n    query: \"Give me a summary about his past experiences?\",\n  });\n\n  console.log({\n    answer: summaryResponse.response,\n    metadata: summaryResponse?.metadata?.selectorResult,\n  });\n\n  const specificResponse = await queryEngine.query({\n    query: \"Tell me about abramov first job?\",\n  });\n\n  console.log({\n    answer: specificResponse.response,\n    metadata: specificResponse.metadata.selectorResult,\n  });\n}\n\nmain().then(() => console.log(\"Done\"));\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "d453d912-94f1-4501-9408-ecb7ec0b6c18": {"__data__": {"id_": "d453d912-94f1-4501-9408-ecb7ec0b6c18", "embedding": null, "metadata": {"filename": "response_synthesizer.md", "extension": ".md", "file_path": "modules/response_synthesizer", "file_name": "response_synthesizer.md", "file_size": 2298, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The ResponseSynthesizer in LlamaIndex generates responses by refining text chunks sequentially or compacting prompts for efficiency. It offers modes like Refine, CompactAndRefine, TreeSummarize, and SimpleResponseBuilder. The synthesize function supports streaming for continuous output. The API includes key features like ResponseSynthesizer, Refine, CompactAndRefine, TreeSummarize, and SimpleResponseBuilder."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 6\n---\n\n# ResponseSynthesizer\n\nThe ResponseSynthesizer is responsible for sending the query, nodes, and prompt templates to the LLM to generate a response. There are a few key modes for generating a response:\n\n- `Refine`: \"create and refine\" an answer by sequentially going through each retrieved text chunk.\n  This makes a separate LLM call per Node. Good for more detailed answers.\n- `CompactAndRefine` (default): \"compact\" the prompt during each LLM call by stuffing as\n  many text chunks that can fit within the maximum prompt size. If there are\n  too many chunks to stuff in one prompt, \"create and refine\" an answer by going through\n  multiple compact prompts. The same as `refine`, but should result in less LLM calls.\n- `TreeSummarize`: Given a set of text chunks and the query, recursively construct a tree\n  and return the root node as the response. Good for summarization purposes.\n- `SimpleResponseBuilder`: Given a set of text chunks and the query, apply the query to each text\n  chunk while accumulating the responses into an array. Returns a concatenated string of all\n  responses. Good for when you need to run the same query separately against each text\n  chunk.\n\n```typescript\nimport { NodeWithScore, ResponseSynthesizer, TextNode } from \"llamaindex\";\n\nconst responseSynthesizer = new ResponseSynthesizer();\n\nconst nodesWithScore: NodeWithScore[] = [\n  {\n    node: new TextNode({ text: \"I am 10 years old.\" }),\n    score: 1,\n  },\n  {\n    node: new TextNode({ text: \"John is 20 years old.\" }),\n    score: 0.5,\n  },\n];\n\nconst response = await responseSynthesizer.synthesize({\n  query: \"What age am I?\",\n  nodesWithScore,\n});\nconsole.log(response.response);\n```\n\nThe `synthesize` function also supports streaming, just add `stream: true` as an option:\n\n```typescript\nconst stream = await responseSynthesizer.synthesize({\n  query: \"What age am I?\",\n  nodesWithScore,\n  stream: true,\n});\nfor await (const chunk of stream) {\n  process.stdout.write(chunk.response);\n}\n```\n\n## API Reference\n\n- ResponseSynthesizer\n- Refine\n- CompactAndRefine\n- TreeSummarize\n- SimpleResponseBuilder\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "403b5c21-3869-40c2-8b2c-e9e7f3225494": {"__data__": {"id_": "403b5c21-3869-40c2-8b2c-e9e7f3225494", "embedding": null, "metadata": {"filename": "retriever.md", "extension": ".md", "file_path": "modules/retriever", "file_name": "retriever.md", "file_size": 684, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex documentation page explains retrievers used to fetch nodes from an index using a query string. It introduces `VectorIndexRetriever` for top-k similar nodes and `SummaryIndexRetriever` for fetching all nodes regardless of the query. The page provides code examples and references to different retriever types available in the API."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 5\n---\n\n# Retriever\n\nA retriever in LlamaIndex is what is used to fetch `Node`s from an index using a query string. Aa `VectorIndexRetriever` will fetch the top-k most similar nodes. Meanwhile, a `SummaryIndexRetriever` will fetch all nodes no matter the query.\n\n```typescript\nconst retriever = vector_index.asRetriever();\nretriever.similarityTopK = 3;\n\n// Fetch nodes!\nconst nodesWithScore = await retriever.retrieve(\"query string\");\n```\n\n## API Reference\n\n- SummaryIndexRetriever\n- SummaryIndexLLMRetriever\n- VectorIndexRetriever\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "bc48899f-3ba2-45c4-8fd4-c056409546e6": {"__data__": {"id_": "bc48899f-3ba2-45c4-8fd4-c056409546e6", "embedding": null, "metadata": {"filename": "storage.md", "extension": ".md", "file_path": "modules/storage", "file_name": "storage.md", "file_size": 695, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The LlamaIndex.TS documentation page explains how to set up storage using a `StorageContext` object with a specified `persistDir`. It details the process of saving and loading data from disk, with future integrations in the pipeline. The provided code snippet demonstrates configuring storage and creating a VectorStoreIndex from documents."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "---\nsidebar_position: 7\n---\n\n# Storage\n\nStorage in LlamaIndex.TS works automatically once you've configured a `StorageContext` object. Just configure the `persistDir` and attach it to an index.\n\nRight now, only saving and loading from disk is supported, with future integrations planned!\n\n```typescript\nimport { Document, VectorStoreIndex, storageContextFromDefaults } from \"./src\";\n\nconst storageContext = await storageContextFromDefaults({\n  persistDir: \"./storage\",\n});\n\nconst document = new Document({ text: \"Test Text\" });\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  storageContext,\n});\n```\n\n## API Reference\n\n- StorageContext\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}, "ec8b5b98-cb1b-4b74-9411-cfe6ca5eae16": {"__data__": {"id_": "ec8b5b98-cb1b-4b74-9411-cfe6ca5eae16", "embedding": null, "metadata": {"filename": "qdrant.md", "extension": ".md", "file_path": "modules/vector_stores/qdrant", "file_name": "qdrant.md", "file_size": 1741, "creation_date": "2024-05-02", "last_modified_date": "2024-05-02", "summary": "The documentation provides a guide on using Qdrant Vector Store with LlamaIndex. It covers setting up a Qdrant instance, importing modules, loading documents, setting up the index, querying the index, and includes a full code example. The focus is on integrating Qdrant for efficient document indexing and querying."}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {}, "text": "# Qdrant Vector Store\n\nTo run this example, you need to have a Qdrant instance running. You can run it with Docker:\n\n```bash\ndocker pull qdrant/qdrant\ndocker run -p 6333:6333 qdrant/qdrant\n```\n\n## Importing the modules\n\n```ts\nimport fs from \"node:fs/promises\";\nimport { Document, VectorStoreIndex, QdrantVectorStore } from \"llamaindex\";\n```\n\n## Load the documents\n\n```ts\nconst path = \"node_modules/llamaindex/examples/abramov.txt\";\nconst essay = await fs.readFile(path, \"utf-8\");\n```\n\n## Setup Qdrant\n\n```ts\nconst vectorStore = new QdrantVectorStore({\n  url: \"http://localhost:6333\",\n});\n```\n\n## Setup the index\n\n```ts\nconst document = new Document({ text: essay, id_: path });\n\nconst index = await VectorStoreIndex.fromDocuments([document], {\n  vectorStore,\n});\n```\n\n## Query the index\n\n```ts\nconst queryEngine = index.asQueryEngine();\n\nconst response = await queryEngine.query({\n  query: \"What did the author do in college?\",\n});\n\n// Output response\nconsole.log(response.toString());\n```\n\n## Full code\n\n```ts\nimport fs from \"node:fs/promises\";\nimport { Document, VectorStoreIndex, QdrantVectorStore } from \"llamaindex\";\n\nasync function main() {\n  const path = \"node_modules/llamaindex/examples/abramov.txt\";\n  const essay = await fs.readFile(path, \"utf-8\");\n\n  const vectorStore = new QdrantVectorStore({\n    url: \"http://localhost:6333\",\n  });\n\n  const document = new Document({ text: essay, id_: path });\n\n  const index = await VectorStoreIndex.fromDocuments([document], {\n    vectorStore,\n  });\n\n  const queryEngine = index.asQueryEngine();\n\n  const response = await queryEngine.query({\n    query: \"What did the author do in college?\",\n  });\n\n  // Output response\n  console.log(response.toString());\n}\n\nmain().catch(console.error);\n```\n", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "Document"}, "__type__": "4"}}}